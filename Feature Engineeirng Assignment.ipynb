{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51ffefe9-51d5-4787-b2e6-4ee8ee4b08a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Q.1] What is a parameter?\\nAns. A parameter refers to a variable that is learned by a model during training. Here's a breakdown of the concept:\\n\\nKey Characteristics of Parameters:\\n\\n  Learned during Training: Parameters are not set manually; they are updated during the training process as the model optimizes its performance.\\n  Direct Influence on Predictions: Parameters define how the model processes input data to make predictions.\\n  Model-Specific: Different models have different types and numbers of parameters.\\n\\nExamples of Parameters in Machine Learning Models:\\n\\n1.) Linear Regression:\\nCoefficients (𝑤) and intercept (𝑏) are the parameters.\\nThey define the relationship between input features and the target variable.\\n                    𝑦 = 𝑤1.𝑥1 + 𝑤2.𝑥2 + ⋯ + 𝑏\\n\\n2.) Neural Networks:\\nWeights and biases for each layer are the parameters.\\nThese determine how inputs are transformed as they pass through the network.\\n\\n3.) Support Vector Machines (SVM):\\nSupport vectors and their associated coefficients are parameters.\\n\\n4.) Clustering (e.g., K-means):\\nCentroid positions are the parameters.\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q.1] What is a parameter?\n",
    "Ans. A parameter refers to a variable that is learned by a model during training. Here's a breakdown of the concept:\n",
    "\n",
    "Key Characteristics of Parameters:\n",
    "\n",
    "  Learned during Training: Parameters are not set manually; they are updated during the training process as the model optimizes its performance.\n",
    "  Direct Influence on Predictions: Parameters define how the model processes input data to make predictions.\n",
    "  Model-Specific: Different models have different types and numbers of parameters.\n",
    "\n",
    "Examples of Parameters in Machine Learning Models:\n",
    "\n",
    "1.) Linear Regression:\n",
    "Coefficients (𝑤) and intercept (𝑏) are the parameters.\n",
    "They define the relationship between input features and the target variable.\n",
    "                    𝑦 = 𝑤1.𝑥1 + 𝑤2.𝑥2 + ⋯ + 𝑏\n",
    "\n",
    "2.) Neural Networks:\n",
    "Weights and biases for each layer are the parameters.\n",
    "These determine how inputs are transformed as they pass through the network.\n",
    "\n",
    "3.) Support Vector Machines (SVM):\n",
    "Support vectors and their associated coefficients are parameters.\n",
    "\n",
    "4.) Clustering (e.g., K-means):\n",
    "Centroid positions are the parameters.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d155896f-d5b2-4001-8bec-302128f77e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q.2] What is correlation? What does negative correlation mean?\\nAns. Correlation is a statistical measure that quantifies the strength and direction of the relationship between two variables. It is often \\nrepresented by the correlation coefficient (𝑟), which ranges from −1 to +1.\\n\\n  1.) Positive Correlation (𝑟 > 0):\\n\\n    As one variable increases, the other variable also increases.\\n    Example: The relationship between height and weight—taller people tend to weigh more.\\n\\n  2.) Negative Correlation (𝑟 < 0):\\n\\n    As one variable increases, the other variable decreases.\\n    Example: The relationship between the number of hours spent studying and the number of errors on a test—more studying leads to fewer errors.\\n\\n  3.) No Correlation (𝑟 = 0):\\n\\n    There is no linear relationship between the variables.\\n    Example: The relationship between shoe size and intelligence.\\n\\n# Negative Correlation : Negative correlation indicates an inverse relationship between two variables. Here’s what it means:\\n\\nWhen one variable increases, the other variable decreases, and vice versa.\\nThe closer the correlation coefficient (𝑟) is to −1, the stronger the negative relationship.\\nExamples:\\n    1. Temperature and Coat Sales : As the temperature decreases, the sales of coats increase.\\n    2. Distance Traveled and Fuel in a Car : As the distance traveled increases, the fuel remaining in the tank decreases.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q.2] What is correlation? What does negative correlation mean?\n",
    "Ans. Correlation is a statistical measure that quantifies the strength and direction of the relationship between two variables. It is often \n",
    "represented by the correlation coefficient (𝑟), which ranges from −1 to +1.\n",
    "\n",
    "  1.) Positive Correlation (𝑟 > 0):\n",
    "\n",
    "    As one variable increases, the other variable also increases.\n",
    "    Example: The relationship between height and weight—taller people tend to weigh more.\n",
    "\n",
    "  2.) Negative Correlation (𝑟 < 0):\n",
    "\n",
    "    As one variable increases, the other variable decreases.\n",
    "    Example: The relationship between the number of hours spent studying and the number of errors on a test—more studying leads to fewer errors.\n",
    "\n",
    "  3.) No Correlation (𝑟 = 0):\n",
    "\n",
    "    There is no linear relationship between the variables.\n",
    "    Example: The relationship between shoe size and intelligence.\n",
    "\n",
    "# Negative Correlation : Negative correlation indicates an inverse relationship between two variables. Here’s what it means:\n",
    "\n",
    "When one variable increases, the other variable decreases, and vice versa.\n",
    "The closer the correlation coefficient (𝑟) is to −1, the stronger the negative relationship.\n",
    "Examples:\n",
    "    1. Temperature and Coat Sales : As the temperature decreases, the sales of coats increase.\n",
    "    2. Distance Traveled and Fuel in a Car : As the distance traveled increases, the fuel remaining in the tank decreases.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e28297ea-7b38-4edf-8efd-9d5e6ec4d6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Q.3] Define Machine Learning. What are the main components in Machine Learning?\\nAns. Machine Learning (ML) is a subset of Artificial Intelligence (AI) that focuses on creating systems that can learn and make decisions or \\npredictions based on data. Instead of being explicitly programmed for every task, ML models are trained on data to identify patterns and relationships, \\nenabling them to generalize and adapt to new, unseen data.\\n\\nFormal Definition: Machine Learning is the study of algorithms and statistical models that enable systems to improve their performance on a specific \\ntask 𝑇, given a dataset 𝐷, and measured by performance metric 𝑃, without explicit instructions.\\n\\nMain Components in Machine Learning\\n\\n1. Data:\\n\\nThe foundation of ML, consisting of features (inputs) and, in supervised learning, labels (outputs).\\nData can be structured (tables, spreadsheets) or unstructured (images, text, audio).\\n\\n2. Features:\\n\\nIndividual measurable properties or characteristics of data. For example:\\nFor housing data: Features could be size, location, and number of bedrooms.\\nFeature engineering and selection play critical roles in improving model performance.\\n\\n3. Model:\\n\\nThe mathematical representation or function that maps inputs (features) to outputs (predictions).\\nExamples of models:\\nLinear regression\\nDecision trees\\nNeural networks\\n\\n4. Algorithm:\\n\\nThe method or procedure used to train the model by optimizing its parameters.\\nExamples:\\nGradient descent\\nSupport vector machine (SVM) optimization\\n\\n5. Training:\\n\\nThe process of feeding data into the model to learn patterns and relationships. Involves splitting the dataset into training, validation, and test sets.\\n\\n6. Loss Function:\\n\\nMeasures the difference between the predicted values and the actual values (ground truth). The goal is to minimize the loss during training.\\nExamples:\\nMean Squared Error (MSE) for regression\\nCross-Entropy Loss for classification\\n\\n7. Optimization:\\n\\nThe process of adjusting the model's parameters to minimize the loss function.\\nOptimization algorithms include:\\nStochastic Gradient Descent (SGD)\\nAdam Optimization (Adaptive Moment Estimation)\\n\\n8. Deployment:\\n\\nMaking the trained model available for use in real-world applications, such as web services or embedded systems.\\n\\n9. Feedback Loop:\\n\\nReal-world applications often incorporate feedback to retrain and improve the model over time.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q.3] Define Machine Learning. What are the main components in Machine Learning?\n",
    "Ans. Machine Learning (ML) is a subset of Artificial Intelligence (AI) that focuses on creating systems that can learn and make decisions or \n",
    "predictions based on data. Instead of being explicitly programmed for every task, ML models are trained on data to identify patterns and relationships, \n",
    "enabling them to generalize and adapt to new, unseen data.\n",
    "\n",
    "Formal Definition: Machine Learning is the study of algorithms and statistical models that enable systems to improve their performance on a specific \n",
    "task 𝑇, given a dataset 𝐷, and measured by performance metric 𝑃, without explicit instructions.\n",
    "\n",
    "Main Components in Machine Learning\n",
    "\n",
    "1. Data:\n",
    "\n",
    "The foundation of ML, consisting of features (inputs) and, in supervised learning, labels (outputs).\n",
    "Data can be structured (tables, spreadsheets) or unstructured (images, text, audio).\n",
    "\n",
    "2. Features:\n",
    "\n",
    "Individual measurable properties or characteristics of data. For example:\n",
    "For housing data: Features could be size, location, and number of bedrooms.\n",
    "Feature engineering and selection play critical roles in improving model performance.\n",
    "\n",
    "3. Model:\n",
    "\n",
    "The mathematical representation or function that maps inputs (features) to outputs (predictions).\n",
    "Examples of models:\n",
    "Linear regression\n",
    "Decision trees\n",
    "Neural networks\n",
    "\n",
    "4. Algorithm:\n",
    "\n",
    "The method or procedure used to train the model by optimizing its parameters.\n",
    "Examples:\n",
    "Gradient descent\n",
    "Support vector machine (SVM) optimization\n",
    "\n",
    "5. Training:\n",
    "\n",
    "The process of feeding data into the model to learn patterns and relationships. Involves splitting the dataset into training, validation, and test sets.\n",
    "\n",
    "6. Loss Function:\n",
    "\n",
    "Measures the difference between the predicted values and the actual values (ground truth). The goal is to minimize the loss during training.\n",
    "Examples:\n",
    "Mean Squared Error (MSE) for regression\n",
    "Cross-Entropy Loss for classification\n",
    "\n",
    "7. Optimization:\n",
    "\n",
    "The process of adjusting the model's parameters to minimize the loss function.\n",
    "Optimization algorithms include:\n",
    "Stochastic Gradient Descent (SGD)\n",
    "Adam Optimization (Adaptive Moment Estimation)\n",
    "\n",
    "8. Deployment:\n",
    "\n",
    "Making the trained model available for use in real-world applications, such as web services or embedded systems.\n",
    "\n",
    "9. Feedback Loop:\n",
    "\n",
    "Real-world applications often incorporate feedback to retrain and improve the model over time.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c87204ca-e932-44d6-b57c-ecd739a6e8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Q.4] How does loss value help in determining whether the model is good or not?\\nAns. The loss value (or loss function) plays a critical role in evaluating how well a machine learning model performs. It quantifies the \\ndifference between the model's predictions and the true values (ground truth). A lower loss value indicates that the model is performing better \\nbecause the predicted values are closer to the actual values, while a higher loss value suggests poor model performance.\\n\\nHere’s how the loss value helps in determining whether the model is good or not:\\n\\n1. Loss as a Measure of Error:\\n   # Loss function measures how far off the predictions are from the true outcomes.\\n   # For regression tasks, the loss could be calculated as the difference between the predicted value and the actual value (e.g., Mean Squared Error or \\n   Mean Absolute Error).\\n   # For classification tasks, the loss could be calculated using metrics like cross-entropy loss, which measures how well the predicted probability \\n   distribution matches the actual class distribution.\\n\\n2. How Loss Guides Model Training:\\n   # Optimization Process: During training, the model adjusts its parameters to minimize the loss function, aiming to reduce the error in predictions. \\n   # Optimizers like Gradient Descent use the loss value to update the model's weights.\\n   # Backpropagation: In deep learning, the loss is propagated back through the network to update weights and improve predictions, lowering the loss \\n   with each iteration.\\n   \\n3. Comparing Loss Across Models:\\n   # Evaluating Model Performance: The loss value can be used to compare different models or hyperparameters. \\n     For example:\\n        A model with a lower loss value is generally considered better, as it indicates closer predictions to the true values.\\n        A higher loss value could suggest that the model is underfitting or not capturing the patterns in the data.\\n   # Generalization: A low training loss but a high validation loss might indicate overfitting, where the model performs well on the training data but \\n     poorly on unseen data. A good model should have both low training and validation losses.\\n\\n4. Types of Loss Functions:\\n   # Regression Loss Functions:\\n        Mean Squared Error (MSE): Measures the average of the squared differences between predicted and actual values.\\n        Mean Absolute Error (MAE): Measures the average of the absolute differences between predicted and actual values.\\n   # Classification Loss Functions:\\n        Cross-Entropy Loss: Measures the difference between two probability distributions — the predicted probabilities and the actual class labels.\\n        Hinge Loss: Used with Support Vector Machines (SVM), measures the margin between predicted class and the actual class.\\n\\n5. Monitoring Loss During Training:\\n   # Convergence: The loss value should decrease steadily as training progresses. If the loss fluctuates or doesn't decrease, this could indicate \\n     issues with learning rate, model architecture, or data quality.\\n   # Validation vs. Training Loss: Monitoring the loss on both the training set and validation set is important:\\n        Decreasing Training Loss: Indicates the model is learning from the data.\\n        Increasing Validation Loss: Suggests overfitting, and techniques like regularization or cross-validation might be needed.\\n\\n6. Importance of Loss in Model Selection:\\n   # Guiding Hyperparameter Tuning: Hyperparameters like the learning rate, batch size, or regularization strength can be adjusted based on the loss \\n     curve.\\n\\n        If the loss is too high, you may adjust the model’s complexity or try different architectures.\\n        If the loss plateaus, adjusting learning rates or optimizer settings might help.\\n   # Early Stopping: Sometimes, you monitor the loss on the validation set and use early stopping to halt training when the loss starts to increase \\n     (indicating overfitting), preventing unnecessary computation.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q.4] How does loss value help in determining whether the model is good or not?\n",
    "Ans. The loss value (or loss function) plays a critical role in evaluating how well a machine learning model performs. It quantifies the \n",
    "difference between the model's predictions and the true values (ground truth). A lower loss value indicates that the model is performing better \n",
    "because the predicted values are closer to the actual values, while a higher loss value suggests poor model performance.\n",
    "\n",
    "Here’s how the loss value helps in determining whether the model is good or not:\n",
    "\n",
    "1. Loss as a Measure of Error:\n",
    "   # Loss function measures how far off the predictions are from the true outcomes.\n",
    "   # For regression tasks, the loss could be calculated as the difference between the predicted value and the actual value (e.g., Mean Squared Error or \n",
    "   Mean Absolute Error).\n",
    "   # For classification tasks, the loss could be calculated using metrics like cross-entropy loss, which measures how well the predicted probability \n",
    "   distribution matches the actual class distribution.\n",
    "\n",
    "2. How Loss Guides Model Training:\n",
    "   # Optimization Process: During training, the model adjusts its parameters to minimize the loss function, aiming to reduce the error in predictions. \n",
    "   # Optimizers like Gradient Descent use the loss value to update the model's weights.\n",
    "   # Backpropagation: In deep learning, the loss is propagated back through the network to update weights and improve predictions, lowering the loss \n",
    "   with each iteration.\n",
    "   \n",
    "3. Comparing Loss Across Models:\n",
    "   # Evaluating Model Performance: The loss value can be used to compare different models or hyperparameters. \n",
    "     For example:\n",
    "        A model with a lower loss value is generally considered better, as it indicates closer predictions to the true values.\n",
    "        A higher loss value could suggest that the model is underfitting or not capturing the patterns in the data.\n",
    "   # Generalization: A low training loss but a high validation loss might indicate overfitting, where the model performs well on the training data but \n",
    "     poorly on unseen data. A good model should have both low training and validation losses.\n",
    "\n",
    "4. Types of Loss Functions:\n",
    "   # Regression Loss Functions:\n",
    "        Mean Squared Error (MSE): Measures the average of the squared differences between predicted and actual values.\n",
    "        Mean Absolute Error (MAE): Measures the average of the absolute differences between predicted and actual values.\n",
    "   # Classification Loss Functions:\n",
    "        Cross-Entropy Loss: Measures the difference between two probability distributions — the predicted probabilities and the actual class labels.\n",
    "        Hinge Loss: Used with Support Vector Machines (SVM), measures the margin between predicted class and the actual class.\n",
    "\n",
    "5. Monitoring Loss During Training:\n",
    "   # Convergence: The loss value should decrease steadily as training progresses. If the loss fluctuates or doesn't decrease, this could indicate \n",
    "     issues with learning rate, model architecture, or data quality.\n",
    "   # Validation vs. Training Loss: Monitoring the loss on both the training set and validation set is important:\n",
    "        Decreasing Training Loss: Indicates the model is learning from the data.\n",
    "        Increasing Validation Loss: Suggests overfitting, and techniques like regularization or cross-validation might be needed.\n",
    "\n",
    "6. Importance of Loss in Model Selection:\n",
    "   # Guiding Hyperparameter Tuning: Hyperparameters like the learning rate, batch size, or regularization strength can be adjusted based on the loss \n",
    "     curve.\n",
    "\n",
    "        If the loss is too high, you may adjust the model’s complexity or try different architectures.\n",
    "        If the loss plateaus, adjusting learning rates or optimizer settings might help.\n",
    "   # Early Stopping: Sometimes, you monitor the loss on the validation set and use early stopping to halt training when the loss starts to increase \n",
    "     (indicating overfitting), preventing unnecessary computation.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95222565-d039-4ef0-886b-05296c783819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Q.5] What are continuous and categorical variables?\\nAns. In data science and machine learning, variables can be broadly classified into two main types: continuous variables and categorical variables. \\nThese two types differ in the nature of the data they represent and how they should be processed.\\n\\n1. Continuous Variables:\\nDefinition: Continuous variables are numeric variables that represent measurable quantities and can take an infinite number of values within a given \\n            range. These variables can be real numbers, meaning they can take any value, including fractions or decimals.\\n\\nExamples:\\n\\n  Height: A person’s height could be 5.8, 5.85, or 5.888 feet, etc.\\n  Temperature: Temperature could be 72.5°F, 73.2°F, or any other value.\\n  Income: A person's income could be $50,000, $50,000.50, or $50,000.01, etc.\\n  Time: The time it takes to perform a task could be 10.5 seconds, 10.55 seconds, etc.\\n\\nProperties:\\n\\n  They can take an infinite number of values within a range.\\n  The values can be measured with any precision, depending on the measuring instrument.\\n  Continuous variables can be summed, averaged, and manipulated using arithmetic operations.\\n\\nMathematical Representation: Continuous variables are typically represented using real numbers and often require numeric models or regression \\n                             techniques in machine learning.\\n\\n2. Categorical Variables:\\nDefinition: Categorical variables, also called qualitative variables, represent categories or groups. These variables can take a limited, fixed number \\n            of possible values, known as categories or levels. They are usually used to represent types, groups, or labels rather than quantities.\\n\\nTypes of Categorical Variables:\\n\\n  Nominal Variables : These are categorical variables with no natural order or ranking between categories.\\n    Examples:\\n      Gender: Male, Female, Non-binary.\\n      Color: Red, Blue, Green.\\n      Country: USA, Canada, India, etc.\\n  Ordinal Variables : These are categorical variables with a natural order or ranking between categories.\\n    Examples:\\n      Education Level: High School, Bachelor's, Master's, PhD.\\n      Rating: Poor, Fair, Good, Excellent.\\n      Age Groups: 18-25, 26-35, 36-45, etc.\\n\\nProperties:\\n\\n  They represent labels or groups that are often non-numeric.\\n  The categories are discrete, and they do not have a meaningful numeric interpretation (except for ordinal variables, which have some inherent order).\\n  They can be encoded into numerical values for use in machine learning algorithms (e.g., one-hot encoding, label encoding).\\n\\nMathematical Representation: Categorical variables can be represented using labels (text) or encoded as numbers (1, 2, 3, etc.) for certain \\n                             algorithms that require numeric input.\\n                             \\nProcessing in Machine Learning:\\n\\nContinuous Variables:\\n  They are often scaled or normalized (e.g., Min-Max Scaling, Z-score standardization) before feeding them into algorithms to ensure that no variable \\n  dominates due to different scales.\\n  Continuous variables are typically used in regression models where predictions are numeric.\\n\\nCategorical Variables:\\n  One-hot Encoding or Label Encoding is used to convert categorical variables into numeric representations.\\n  Ordinal Encoding can be used for ordinal variables where the order matters.\\n  Categorical variables are used in classification models, where the goal is to predict categories or classes.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q.5] What are continuous and categorical variables?\n",
    "Ans. In data science and machine learning, variables can be broadly classified into two main types: continuous variables and categorical variables. \n",
    "These two types differ in the nature of the data they represent and how they should be processed.\n",
    "\n",
    "1. Continuous Variables:\n",
    "Definition: Continuous variables are numeric variables that represent measurable quantities and can take an infinite number of values within a given \n",
    "            range. These variables can be real numbers, meaning they can take any value, including fractions or decimals.\n",
    "\n",
    "Examples:\n",
    "\n",
    "  Height: A person’s height could be 5.8, 5.85, or 5.888 feet, etc.\n",
    "  Temperature: Temperature could be 72.5°F, 73.2°F, or any other value.\n",
    "  Income: A person's income could be $50,000, $50,000.50, or $50,000.01, etc.\n",
    "  Time: The time it takes to perform a task could be 10.5 seconds, 10.55 seconds, etc.\n",
    "\n",
    "Properties:\n",
    "\n",
    "  They can take an infinite number of values within a range.\n",
    "  The values can be measured with any precision, depending on the measuring instrument.\n",
    "  Continuous variables can be summed, averaged, and manipulated using arithmetic operations.\n",
    "\n",
    "Mathematical Representation: Continuous variables are typically represented using real numbers and often require numeric models or regression \n",
    "                             techniques in machine learning.\n",
    "\n",
    "2. Categorical Variables:\n",
    "Definition: Categorical variables, also called qualitative variables, represent categories or groups. These variables can take a limited, fixed number \n",
    "            of possible values, known as categories or levels. They are usually used to represent types, groups, or labels rather than quantities.\n",
    "\n",
    "Types of Categorical Variables:\n",
    "\n",
    "  Nominal Variables : These are categorical variables with no natural order or ranking between categories.\n",
    "    Examples:\n",
    "      Gender: Male, Female, Non-binary.\n",
    "      Color: Red, Blue, Green.\n",
    "      Country: USA, Canada, India, etc.\n",
    "  Ordinal Variables : These are categorical variables with a natural order or ranking between categories.\n",
    "    Examples:\n",
    "      Education Level: High School, Bachelor's, Master's, PhD.\n",
    "      Rating: Poor, Fair, Good, Excellent.\n",
    "      Age Groups: 18-25, 26-35, 36-45, etc.\n",
    "\n",
    "Properties:\n",
    "\n",
    "  They represent labels or groups that are often non-numeric.\n",
    "  The categories are discrete, and they do not have a meaningful numeric interpretation (except for ordinal variables, which have some inherent order).\n",
    "  They can be encoded into numerical values for use in machine learning algorithms (e.g., one-hot encoding, label encoding).\n",
    "\n",
    "Mathematical Representation: Categorical variables can be represented using labels (text) or encoded as numbers (1, 2, 3, etc.) for certain \n",
    "                             algorithms that require numeric input.\n",
    "                             \n",
    "Processing in Machine Learning:\n",
    "\n",
    "Continuous Variables:\n",
    "  They are often scaled or normalized (e.g., Min-Max Scaling, Z-score standardization) before feeding them into algorithms to ensure that no variable \n",
    "  dominates due to different scales.\n",
    "  Continuous variables are typically used in regression models where predictions are numeric.\n",
    "\n",
    "Categorical Variables:\n",
    "  One-hot Encoding or Label Encoding is used to convert categorical variables into numeric representations.\n",
    "  Ordinal Encoding can be used for ordinal variables where the order matters.\n",
    "  Categorical variables are used in classification models, where the goal is to predict categories or classes.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd4d4ea6-2e80-417b-a6d9-a475b977bc2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q.6] How do we handle categorical variables in Machine Learning? What are the common techniques?\\nAns. Categorical variables need to be transformed into a format that machine learning models can interpret, since most models expect numeric input. \\nThere are several techniques to handle categorical data, each suited to different types of categorical variables (e.g., nominal vs. ordinal). \\nHere are the common techniques used to handle categorical variables:\\n\\n1. Label Encoding (for Ordinal Data)\\nDefinition: Label encoding involves assigning each category in a categorical variable a unique numeric label. This technique is best used when the \\ncategorical variable is ordinal, meaning the categories have a meaningful order or ranking.\\n\\nExample : Education Level: High School = 0, Bachelor\\'s = 1, Master\\'s = 2, PhD = 3.\\n\\nImplementation : Each category is mapped to an integer value.\\n\\nAdvantages:\\n\\n  Simple and fast.\\n  Useful when the categorical feature has a natural order (ordinal).\\n\\nDisadvantages:\\n\\nIt introduces an artificial ordinal relationship for nominal data (non-ordered categories), which could be problematic for machine learning algorithms.\\n\\n2. One-Hot Encoding (for Nominal Data)\\nDefinition: One-hot encoding is the most common method for handling nominal categorical variables (i.e., categories without an inherent order). \\nEach category in a categorical feature is represented as a binary vector, where each category gets its own column, and the value is either 0 or 1 \\ndepending on whether the observation belongs to that category.\\n\\nExample:\\n\\nColor: Red, Blue, Green → Three new columns:\\nRed: [1, 0, 0],\\nBlue: [0, 1, 0],\\nGreen: [0, 0, 1].\\n\\nImplementation:\\n    Convert each unique category into a separate column.\\n    For each row, set the column corresponding to the category to 1 and all others to 0.\\n\\nAdvantages:\\n    Does not introduce any ordinal relationships; treats categories as independent.\\n    Works well with nominal data.\\n\\nDisadvantages:\\n    Curse of Dimensionality: It increases the feature space, especially when the categorical variable has many unique categories (e.g., many cities or \\n                     products).\\n    Can lead to sparse matrices with many zeros.\\n\\n3. Binary Encoding (for High Cardinality)\\nDefinition: Binary encoding is used when a categorical variable has a high cardinality (many unique categories). This technique converts categories \\n            into binary numbers and represents them as separate columns.\\n\\nExample:\\n    Suppose a feature has four categories: Red, Blue, Green, Yellow. Using binary encoding, we convert them into binary numbers (e.g., 00, 01, 10, 11), \\n    and each category will be represented by two binary columns.\\n\\nImplementation:\\n    First, assign an integer label to each category.\\n    Convert the integer labels into binary values and create new columns to represent the binary digits.\\n\\nAdvantages:\\n    Efficient for high-cardinality categorical features, as it reduces dimensionality compared to one-hot encoding.\\n    Keeps the feature space smaller than one-hot encoding.\\n\\nDisadvantages:\\n    The binary values may still be misinterpreted by certain models that expect continuous relationships, so it\\'s important to assess the impact on \\n    your model.\\n\\n4. Frequency or Count Encoding\\nDefinition: Frequency or count encoding replaces each category with the frequency or count of its occurrences in the dataset. \\n            It works well when the importance of a category depends on how frequently it occurs in the data.\\n\\nExample:\\n    Country: USA (50), Canada (30), India (20), etc.\\n    Each country is replaced by the number of times it appears in the dataset.\\n\\nImplementation:\\n    Replace each category with the count (or frequency) of its occurrences in the dataset.\\n\\nAdvantages:\\n    Simpler and more efficient for categories with many values.\\n    Useful when frequency provides predictive information.\\n\\nDisadvantages:\\n    The numerical encoding can introduce a bias toward more frequent categories.\\n\\n5. Target (Mean) Encoding\\nDefinition: Target encoding, or mean encoding, involves replacing categories with the mean of the target variable for each category. It is commonly \\n            used in supervised learning, where you encode a categorical feature based on the mean of the target variable (e.g., regression or \\n            classification).\\n\\nExample:\\n    If the target is a continuous variable, the feature \"City\" can be encoded by the average target value for each city.\\n\\nImplementation:\\n    For each category, replace the category with the mean of the target variable corresponding to that category.\\n\\nAdvantages:\\n    Can be very powerful if the encoding relates to the target variable.\\n    Useful in cases where the categories may have a strong relationship with the target.\\n\\nDisadvantages:\\n    Can lead to overfitting, especially when there are many categories and few observations. To prevent overfitting, techniques like smoothing or \\n    cross-validation can be used.\\n    Not suitable for categorical variables that do not relate to the target.\\n\\n6. Embedding Layers (for Neural Networks)\\nDefinition: Embedding layers are used when categorical variables are high-dimensional (e.g., words in text or items in a recommendation system). \\n            An embedding layer maps each category to a dense vector of continuous values, and these vectors are learned during model training.\\n\\nExample:\\n    In natural language processing (NLP), words are represented as vectors (e.g., Word2Vec embeddings).\\n\\nImplementation:\\n    Use the embedding layer in a neural network to represent each category as a continuous vector.\\n\\nAdvantages:\\n    Embedding layers can represent high-cardinality categorical variables in a compact form.\\n    They allow the model to learn better representations of the categories.\\n\\nDisadvantages:\\n    Computationally expensive.\\n    Requires more advanced deep learning techniques.\\n\\n7. Hashing (for High Cardinality)\\nDefinition: Hashing is used to transform categorical variables into a fixed-size vector using a hash function. It is useful for high-cardinality data, \\n            where one-hot encoding would create too many columns.\\n\\nExample:\\n    Instead of creating columns for each category, the hash function will map categories to a fixed number of columns (buckets).\\n\\nImplementation:\\n    Apply a hash function to map the categories to a specific number of buckets.\\n\\nAdvantages:\\n    Efficient when dealing with a large number of categories.\\n    Reduces the dimensionality when categories are too numerous.\\n\\nDisadvantages:\\n    There may be collisions where different categories get mapped to the same bucket.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q.6] How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
    "Ans. Categorical variables need to be transformed into a format that machine learning models can interpret, since most models expect numeric input. \n",
    "There are several techniques to handle categorical data, each suited to different types of categorical variables (e.g., nominal vs. ordinal). \n",
    "Here are the common techniques used to handle categorical variables:\n",
    "\n",
    "1. Label Encoding (for Ordinal Data)\n",
    "Definition: Label encoding involves assigning each category in a categorical variable a unique numeric label. This technique is best used when the \n",
    "categorical variable is ordinal, meaning the categories have a meaningful order or ranking.\n",
    "\n",
    "Example : Education Level: High School = 0, Bachelor's = 1, Master's = 2, PhD = 3.\n",
    "\n",
    "Implementation : Each category is mapped to an integer value.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "  Simple and fast.\n",
    "  Useful when the categorical feature has a natural order (ordinal).\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "It introduces an artificial ordinal relationship for nominal data (non-ordered categories), which could be problematic for machine learning algorithms.\n",
    "\n",
    "2. One-Hot Encoding (for Nominal Data)\n",
    "Definition: One-hot encoding is the most common method for handling nominal categorical variables (i.e., categories without an inherent order). \n",
    "Each category in a categorical feature is represented as a binary vector, where each category gets its own column, and the value is either 0 or 1 \n",
    "depending on whether the observation belongs to that category.\n",
    "\n",
    "Example:\n",
    "\n",
    "Color: Red, Blue, Green → Three new columns:\n",
    "Red: [1, 0, 0],\n",
    "Blue: [0, 1, 0],\n",
    "Green: [0, 0, 1].\n",
    "\n",
    "Implementation:\n",
    "    Convert each unique category into a separate column.\n",
    "    For each row, set the column corresponding to the category to 1 and all others to 0.\n",
    "\n",
    "Advantages:\n",
    "    Does not introduce any ordinal relationships; treats categories as independent.\n",
    "    Works well with nominal data.\n",
    "\n",
    "Disadvantages:\n",
    "    Curse of Dimensionality: It increases the feature space, especially when the categorical variable has many unique categories (e.g., many cities or \n",
    "                     products).\n",
    "    Can lead to sparse matrices with many zeros.\n",
    "\n",
    "3. Binary Encoding (for High Cardinality)\n",
    "Definition: Binary encoding is used when a categorical variable has a high cardinality (many unique categories). This technique converts categories \n",
    "            into binary numbers and represents them as separate columns.\n",
    "\n",
    "Example:\n",
    "    Suppose a feature has four categories: Red, Blue, Green, Yellow. Using binary encoding, we convert them into binary numbers (e.g., 00, 01, 10, 11), \n",
    "    and each category will be represented by two binary columns.\n",
    "\n",
    "Implementation:\n",
    "    First, assign an integer label to each category.\n",
    "    Convert the integer labels into binary values and create new columns to represent the binary digits.\n",
    "\n",
    "Advantages:\n",
    "    Efficient for high-cardinality categorical features, as it reduces dimensionality compared to one-hot encoding.\n",
    "    Keeps the feature space smaller than one-hot encoding.\n",
    "\n",
    "Disadvantages:\n",
    "    The binary values may still be misinterpreted by certain models that expect continuous relationships, so it's important to assess the impact on \n",
    "    your model.\n",
    "\n",
    "4. Frequency or Count Encoding\n",
    "Definition: Frequency or count encoding replaces each category with the frequency or count of its occurrences in the dataset. \n",
    "            It works well when the importance of a category depends on how frequently it occurs in the data.\n",
    "\n",
    "Example:\n",
    "    Country: USA (50), Canada (30), India (20), etc.\n",
    "    Each country is replaced by the number of times it appears in the dataset.\n",
    "\n",
    "Implementation:\n",
    "    Replace each category with the count (or frequency) of its occurrences in the dataset.\n",
    "\n",
    "Advantages:\n",
    "    Simpler and more efficient for categories with many values.\n",
    "    Useful when frequency provides predictive information.\n",
    "\n",
    "Disadvantages:\n",
    "    The numerical encoding can introduce a bias toward more frequent categories.\n",
    "\n",
    "5. Target (Mean) Encoding\n",
    "Definition: Target encoding, or mean encoding, involves replacing categories with the mean of the target variable for each category. It is commonly \n",
    "            used in supervised learning, where you encode a categorical feature based on the mean of the target variable (e.g., regression or \n",
    "            classification).\n",
    "\n",
    "Example:\n",
    "    If the target is a continuous variable, the feature \"City\" can be encoded by the average target value for each city.\n",
    "\n",
    "Implementation:\n",
    "    For each category, replace the category with the mean of the target variable corresponding to that category.\n",
    "\n",
    "Advantages:\n",
    "    Can be very powerful if the encoding relates to the target variable.\n",
    "    Useful in cases where the categories may have a strong relationship with the target.\n",
    "\n",
    "Disadvantages:\n",
    "    Can lead to overfitting, especially when there are many categories and few observations. To prevent overfitting, techniques like smoothing or \n",
    "    cross-validation can be used.\n",
    "    Not suitable for categorical variables that do not relate to the target.\n",
    "\n",
    "6. Embedding Layers (for Neural Networks)\n",
    "Definition: Embedding layers are used when categorical variables are high-dimensional (e.g., words in text or items in a recommendation system). \n",
    "            An embedding layer maps each category to a dense vector of continuous values, and these vectors are learned during model training.\n",
    "\n",
    "Example:\n",
    "    In natural language processing (NLP), words are represented as vectors (e.g., Word2Vec embeddings).\n",
    "\n",
    "Implementation:\n",
    "    Use the embedding layer in a neural network to represent each category as a continuous vector.\n",
    "\n",
    "Advantages:\n",
    "    Embedding layers can represent high-cardinality categorical variables in a compact form.\n",
    "    They allow the model to learn better representations of the categories.\n",
    "\n",
    "Disadvantages:\n",
    "    Computationally expensive.\n",
    "    Requires more advanced deep learning techniques.\n",
    "\n",
    "7. Hashing (for High Cardinality)\n",
    "Definition: Hashing is used to transform categorical variables into a fixed-size vector using a hash function. It is useful for high-cardinality data, \n",
    "            where one-hot encoding would create too many columns.\n",
    "\n",
    "Example:\n",
    "    Instead of creating columns for each category, the hash function will map categories to a fixed number of columns (buckets).\n",
    "\n",
    "Implementation:\n",
    "    Apply a hash function to map the categories to a specific number of buckets.\n",
    "\n",
    "Advantages:\n",
    "    Efficient when dealing with a large number of categories.\n",
    "    Reduces the dimensionality when categories are too numerous.\n",
    "\n",
    "Disadvantages:\n",
    "    There may be collisions where different categories get mapped to the same bucket.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2932cf66-c885-4a41-9903-a2c5ae5dd009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q.7] What do you mean by training and testing a dataset?\\nAns. In machine learning, we often split the available dataset into two parts: the training set and the testing set. The purpose of this split is to \\ntrain the model on one subset of data and evaluate its performance on another, unseen subset, which helps ensure that the model generalizes well to \\nnew data.\\n\\nHere’s a breakdown of what training and testing a dataset mean:\\n\\n1. Training a Dataset:\\nDefinition: Training involves feeding the training set into the machine learning algorithm to help the model learn patterns from the data. \\n    The algorithm adjusts the model’s internal parameters (e.g., weights in neural networks, coefficients in regression models) based on the features \\n    of the training data and the corresponding target values (labels or outcomes).\\n\\nProcess:\\n    The training set consists of input data (features) along with their correct output (labels for supervised learning).\\n    The model makes predictions based on the input features, and the predictions are compared to the actual labels.\\n    The error or loss is calculated (e.g., Mean Squared Error for regression or Cross-Entropy for classification).\\n    The model uses this error to update its parameters through an optimization algorithm (like gradient descent), aiming to minimize the error.\\n\\nGoal: The goal of training is to enable the model to learn the relationship between input features and target variables so that it can make \\n      accurate predictions on new, unseen data.\\n\\n2. Testing a Dataset:\\nDefinition: Testing involves evaluating the model on the testing set, which was not used during training. The purpose of testing is to assess how \\n  well the trained model performs on unseen data, providing a measure of its generalization capability.\\n\\nProcess:\\n    After the model has been trained on the training set, it is tested on the testing set.\\n    The testing set consists of data that the model has never seen before, and it includes the input features along with their corresponding target \\n    labels.\\n    The model makes predictions on the testing set, and its predictions are compared to the actual labels.\\n    The accuracy or other performance metrics (e.g., Precision, Recall, F1-score for classification, or RMSE for regression) are calculated to evaluate \\n    the model’s effectiveness.\\n\\nGoal: The goal of testing is to check how well the model generalizes to new data. This helps ensure that the model hasn’t simply memorized the \\n      training data (overfitting) and can instead apply learned patterns to new, unseen data.\\n\\n3. Why Split the Dataset?\\nThe split between training and testing sets serves several purposes:\\n\\n    Avoid Overfitting: If the model is trained and tested on the same data, it may memorize the training data, leading to overfitting. This means the \\n    model performs well on the training set but poorly on unseen data. By testing on a separate set, we ensure that the model can generalize well to \\n    new, unseen examples.\\n\\n    Model Evaluation: The testing set allows us to evaluate how well the model is likely to perform on real-world data.\\n\\n    Hyperparameter Tuning: In practice, we may use a third dataset called the validation set to fine-tune hyperparameters (e.g., learning rate, \\n                       regularization). After fine-tuning on the validation set, we test the model on the testing set to check its performance.\\n\\n4. Common Data Splits:\\n    Typical Split: Common splits are 80% training / 20% testing or 70% training / 30% testing, though the exact split can vary based on the dataset size \\n                   and application.\\n\\n    Cross-Validation: If the dataset is small, techniques like k-fold cross-validation can be used to evaluate the model more effectively. In k-fold \\n    cross-validation, the dataset is divided into k folds. For each iteration, the model is trained on k-1 folds (training set) and tested on the \\n    remaining fold (test set). This process is repeated k times, ensuring that every fold is used once as the test set and k-1 times as part of the \\n    training set.\\n\\n5. Practical Example:\\nImagine you are building a machine learning model to predict whether a customer will buy a product based on features like age, income, and browsing \\nhistory:\\n\\n    Training the Model: You train your model on a dataset with features (age, income, browsing history) and the target label (whether or not they bought \\n            the product). The model learns the relationship between these features and the target.\\n\\n    Testing the Model: After training, you test the model on a different set of data that it hasn’t seen before (testing set). The model predicts \\n            whether customers will buy the product, and its predictions are compared to the actual outcomes to assess its performance.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q.7] What do you mean by training and testing a dataset?\n",
    "Ans. In machine learning, we often split the available dataset into two parts: the training set and the testing set. The purpose of this split is to \n",
    "train the model on one subset of data and evaluate its performance on another, unseen subset, which helps ensure that the model generalizes well to \n",
    "new data.\n",
    "\n",
    "Here’s a breakdown of what training and testing a dataset mean:\n",
    "\n",
    "1. Training a Dataset:\n",
    "Definition: Training involves feeding the training set into the machine learning algorithm to help the model learn patterns from the data. \n",
    "    The algorithm adjusts the model’s internal parameters (e.g., weights in neural networks, coefficients in regression models) based on the features \n",
    "    of the training data and the corresponding target values (labels or outcomes).\n",
    "\n",
    "Process:\n",
    "    The training set consists of input data (features) along with their correct output (labels for supervised learning).\n",
    "    The model makes predictions based on the input features, and the predictions are compared to the actual labels.\n",
    "    The error or loss is calculated (e.g., Mean Squared Error for regression or Cross-Entropy for classification).\n",
    "    The model uses this error to update its parameters through an optimization algorithm (like gradient descent), aiming to minimize the error.\n",
    "\n",
    "Goal: The goal of training is to enable the model to learn the relationship between input features and target variables so that it can make \n",
    "      accurate predictions on new, unseen data.\n",
    "\n",
    "2. Testing a Dataset:\n",
    "Definition: Testing involves evaluating the model on the testing set, which was not used during training. The purpose of testing is to assess how \n",
    "  well the trained model performs on unseen data, providing a measure of its generalization capability.\n",
    "\n",
    "Process:\n",
    "    After the model has been trained on the training set, it is tested on the testing set.\n",
    "    The testing set consists of data that the model has never seen before, and it includes the input features along with their corresponding target \n",
    "    labels.\n",
    "    The model makes predictions on the testing set, and its predictions are compared to the actual labels.\n",
    "    The accuracy or other performance metrics (e.g., Precision, Recall, F1-score for classification, or RMSE for regression) are calculated to evaluate \n",
    "    the model’s effectiveness.\n",
    "\n",
    "Goal: The goal of testing is to check how well the model generalizes to new data. This helps ensure that the model hasn’t simply memorized the \n",
    "      training data (overfitting) and can instead apply learned patterns to new, unseen data.\n",
    "\n",
    "3. Why Split the Dataset?\n",
    "The split between training and testing sets serves several purposes:\n",
    "\n",
    "    Avoid Overfitting: If the model is trained and tested on the same data, it may memorize the training data, leading to overfitting. This means the \n",
    "    model performs well on the training set but poorly on unseen data. By testing on a separate set, we ensure that the model can generalize well to \n",
    "    new, unseen examples.\n",
    "\n",
    "    Model Evaluation: The testing set allows us to evaluate how well the model is likely to perform on real-world data.\n",
    "\n",
    "    Hyperparameter Tuning: In practice, we may use a third dataset called the validation set to fine-tune hyperparameters (e.g., learning rate, \n",
    "                       regularization). After fine-tuning on the validation set, we test the model on the testing set to check its performance.\n",
    "\n",
    "4. Common Data Splits:\n",
    "    Typical Split: Common splits are 80% training / 20% testing or 70% training / 30% testing, though the exact split can vary based on the dataset size \n",
    "                   and application.\n",
    "\n",
    "    Cross-Validation: If the dataset is small, techniques like k-fold cross-validation can be used to evaluate the model more effectively. In k-fold \n",
    "    cross-validation, the dataset is divided into k folds. For each iteration, the model is trained on k-1 folds (training set) and tested on the \n",
    "    remaining fold (test set). This process is repeated k times, ensuring that every fold is used once as the test set and k-1 times as part of the \n",
    "    training set.\n",
    "\n",
    "5. Practical Example:\n",
    "Imagine you are building a machine learning model to predict whether a customer will buy a product based on features like age, income, and browsing \n",
    "history:\n",
    "\n",
    "    Training the Model: You train your model on a dataset with features (age, income, browsing history) and the target label (whether or not they bought \n",
    "            the product). The model learns the relationship between these features and the target.\n",
    "\n",
    "    Testing the Model: After training, you test the model on a different set of data that it hasn’t seen before (testing set). The model predicts \n",
    "            whether customers will buy the product, and its predictions are compared to the actual outcomes to assess its performance.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3079981-4fef-4608-a9c1-b3dc68ee23e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q.8] What is sklearn.preprocessing?\\nAns. sklearn.preprocessing is a module in the Scikit-learn library that provides tools for data preprocessing in machine learning pipelines. \\nPreprocessing involves transforming raw data into a format suitable for training and testing machine learning models. This step is critical to \\nensure that the model performs optimally and generalizes well to unseen data.\\n\\nKey Features of sklearn.preprocessing\\n\\n    1. Scaling : Ensures that numerical features have a consistent scale, which is important for algorithms that are sensitive to feature magnitudes \\n                 (e.g., SVMs, k-means, neural networks).\\n\\n    2. Encoding : Converts categorical variables into numeric formats suitable for machine learning models.\\n\\n    3. Imputation : Handles missing values in the dataset.\\n\\n    4. Feature Transformation : Applies mathematical transformations to features to improve their compatibility with the learning algorithm.\\n\\nWhen to Use sklearn.preprocessing?\\n    When data has different scales or units (e.g., height in cm and weight in kg).\\n    To handle categorical features.\\n    To deal with missing data.\\n    When applying dimensionality reduction or feature engineering.\\n    To ensure data is in a format compatible with machine learning models.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q.8] What is sklearn.preprocessing?\n",
    "Ans. sklearn.preprocessing is a module in the Scikit-learn library that provides tools for data preprocessing in machine learning pipelines. \n",
    "Preprocessing involves transforming raw data into a format suitable for training and testing machine learning models. This step is critical to \n",
    "ensure that the model performs optimally and generalizes well to unseen data.\n",
    "\n",
    "Key Features of sklearn.preprocessing\n",
    "\n",
    "    1. Scaling : Ensures that numerical features have a consistent scale, which is important for algorithms that are sensitive to feature magnitudes \n",
    "                 (e.g., SVMs, k-means, neural networks).\n",
    "\n",
    "    2. Encoding : Converts categorical variables into numeric formats suitable for machine learning models.\n",
    "\n",
    "    3. Imputation : Handles missing values in the dataset.\n",
    "\n",
    "    4. Feature Transformation : Applies mathematical transformations to features to improve their compatibility with the learning algorithm.\n",
    "\n",
    "When to Use sklearn.preprocessing?\n",
    "    When data has different scales or units (e.g., height in cm and weight in kg).\n",
    "    To handle categorical features.\n",
    "    To deal with missing data.\n",
    "    When applying dimensionality reduction or feature engineering.\n",
    "    To ensure data is in a format compatible with machine learning models.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "233a6c5b-31a2-45b0-9562-3967a1a59d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q.9] What is a Test set?\\nAns. The test set is a subset of the dataset used to evaluate the performance of a trained machine learning model. It consists of data that the \\nmodel has never seen during the training process. Testing the model on this unseen data provides an unbiased assessment of its ability to generalize \\nto new, real-world data.\\n\\nPurpose of the Test Set\\n    Evaluate Generalization:\\n        The test set helps determine how well the model performs on data that it was not trained on.\\n        This ensures that the model is not overfitting (memorizing the training data) or underfitting (failing to capture the underlying patterns).\\n\\n    Measure Performance : Metrics such as accuracy, precision, recall, F1-score (for classification), or Mean Squared Error (MSE) and R² (for regression)\\n                      are computed on the test set to quantify the model’s performance.\\n\\n    Compare Models : The test set allows comparison between different models or different configurations of the same model to select the best one.\\n\\nKey Characteristics of a Test Set:\\n\\n    Unseen by the Model During Training:\\n        The test set is completely separate from the training data to ensure an unbiased evaluation of the model\\'s performance.\\n\\n    Used for Evaluation, Not Learning:\\n        The model does not use the test set to adjust its parameters. It is only used to assess how well the model generalizes.\\n\\n    Represents Real-World Data:\\n        Ideally, the test set should reflect the type of data the model is expected to encounter in production.\\n\\n    Fixed Once Chosen:\\n        The test set should remain the same throughout the development process to avoid \"leakage\" or biased results from repeated evaluations.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q.9] What is a Test set?\n",
    "Ans. The test set is a subset of the dataset used to evaluate the performance of a trained machine learning model. It consists of data that the \n",
    "model has never seen during the training process. Testing the model on this unseen data provides an unbiased assessment of its ability to generalize \n",
    "to new, real-world data.\n",
    "\n",
    "Purpose of the Test Set\n",
    "    Evaluate Generalization:\n",
    "        The test set helps determine how well the model performs on data that it was not trained on.\n",
    "        This ensures that the model is not overfitting (memorizing the training data) or underfitting (failing to capture the underlying patterns).\n",
    "\n",
    "    Measure Performance : Metrics such as accuracy, precision, recall, F1-score (for classification), or Mean Squared Error (MSE) and R² (for regression)\n",
    "                      are computed on the test set to quantify the model’s performance.\n",
    "\n",
    "    Compare Models : The test set allows comparison between different models or different configurations of the same model to select the best one.\n",
    "\n",
    "Key Characteristics of a Test Set:\n",
    "\n",
    "    Unseen by the Model During Training:\n",
    "        The test set is completely separate from the training data to ensure an unbiased evaluation of the model's performance.\n",
    "\n",
    "    Used for Evaluation, Not Learning:\n",
    "        The model does not use the test set to adjust its parameters. It is only used to assess how well the model generalizes.\n",
    "\n",
    "    Represents Real-World Data:\n",
    "        Ideally, the test set should reflect the type of data the model is expected to encounter in production.\n",
    "\n",
    "    Fixed Once Chosen:\n",
    "        The test set should remain the same throughout the development process to avoid \"leakage\" or biased results from repeated evaluations.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba18e8c4-79cf-422b-be7c-605d81ce8998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Splitting data for model fitting\n",
      "Training features: [[5], [3], [1], [4]]\n",
      "Testing features: [[2]]\n",
      "Train: [1 2 3 4] Test: [0]\n",
      "Train: [0 2 3 4] Test: [1]\n",
      "Train: [0 1 3 4] Test: [2]\n",
      "Train: [0 1 2 4] Test: [3]\n",
      "Train: [0 1 2 3] Test: [4]\n"
     ]
    }
   ],
   "source": [
    "'''Q.10] How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?\n",
    "Ans. Splitting data into training and testing sets is a standard practice in machine learning. It helps evaluate the model's ability to generalize \n",
    "to unseen data. Here's how you can do it in Python using Scikit-learn:\n",
    "\n",
    "Using train_test_split from Scikit-learn\n",
    "The train_test_split function is the most common method to split datasets.'''\n",
    "print(\"For Splitting data for model fitting\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example dataset\n",
    "X = [[1], [2], [3], [4], [5]]  # Features\n",
    "y = [1, 2, 3, 4, 5]            # Labels or target\n",
    "\n",
    "# Split the data into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training features:\", X_train)\n",
    "print(\"Testing features:\", X_test)\n",
    "\n",
    "'''Parameters in train_test_split:\n",
    "test_size: Proportion of the dataset to include in the test split (e.g., 0.2 for 20%).\n",
    "random_state: Ensures reproducibility by controlling the random shuffling of data.\n",
    "stratify: Maintains the distribution of classes in classification tasks.\n",
    "\n",
    "Cross-Validation with KFold\n",
    "For better evaluation, you can use k-fold cross-validation instead of a single train-test split.'''\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "\n",
    "'''2. How to Approach a Machine Learning Problem\n",
    "Step-by-Step Approach:\n",
    "  1. Understand the Problem:\n",
    "    Define the problem (e.g., regression, classification, clustering).\n",
    "    Understand the business context and objectives.\n",
    "  \n",
    "  2. Explore the Data:\n",
    "    Load the dataset (e.g., using pandas).\n",
    "    Analyze features, target variable, and distributions.\n",
    "    Identify missing values, outliers, and correlations\n",
    "    \n",
    "  3. Preprocess the Data:\n",
    "    Handle missing values (e.g., SimpleImputer).\n",
    "    Encode categorical variables (e.g., OneHotEncoder or LabelEncoder).\n",
    "    Scale or normalize numerical features (e.g., StandardScaler).\n",
    "\n",
    "  4. Split the Dataset:\n",
    "    Split the data into training, validation, and test sets.\n",
    "    \n",
    "  5. Choose a Model:\n",
    "    Select appropriate machine learning algorithms based on the problem type and data.\n",
    "    Regression: Linear Regression, Decision Trees, etc.\n",
    "    Classification: Logistic Regression, Random Forest, SVM, etc.\n",
    "    Clustering: K-Means, DBSCAN, etc.\n",
    "\n",
    "  6. Train the Model : Fit the chosen algorithm to the training data.\n",
    "\n",
    "  7. Validate the Model:\n",
    "    Use the validation set to fine-tune hyperparameters and assess the model's performance.\n",
    "    Use cross-validation for robust evaluation.\n",
    "\n",
    "  8. Evaluate on Test Set : After finalizing the model, evaluate it on the test set.\n",
    "\n",
    "  9. Optimize the Model:\n",
    "    Experiment with feature selection, engineering, and hyperparameter tuning (e.g., GridSearchCV or RandomizedSearchCV).\n",
    "\n",
    "  10. Deploy the Model : Save the model (e.g., using joblib or pickle) and deploy it to production.\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf4f11f2-12d7-49e3-b136-a053bbda7104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Q.11] Why do we have to perform EDA before fitting a model to the data?\\nAns. Performing Exploratory Data Analysis (EDA) before fitting a machine learning model is essential because it helps you understand the dataset, \\nidentify potential issues, and make informed decisions about preprocessing, feature engineering, and model selection. Here's why EDA is critical:\\n\\n1. Understand the Data\\nInsight into Features:\\n    EDA provides an overview of the dataset, such as the number of features, types of variables (categorical or numerical), and distributions.\\n\\nIt answers questions like:\\n  Are the features continuous, categorical, or binary?\\n  What is the range of the values?\\n\\nTarget Variable:\\n  For supervised learning, understanding the target variable is crucial. For example:\\n    Is the target variable balanced?\\n    Does it have outliers?\\n\\n2. Detect Missing Values\\nIdentify Missing Data:\\n  Missing values can lead to errors or biased model training. EDA helps identify the presence and extent of missing data.\\n  Example: If 80% of a feature's values are missing, it might be better to drop that feature.\\n\\nPlan Imputation:\\n  Decide whether to impute missing values (e.g., mean, median) or use advanced techniques like KNNImputer.\\n\\n3. Spot Outliers\\nOutlier Detection:\\n  EDA helps identify extreme values that could skew the model.\\n  Example: In a regression model, large outliers in the target variable might dominate the loss function.\\n\\nHandle Appropriately:\\n  Decide whether to remove, cap, or transform outliers.\\n\\n4. Explore Relationships Between Variables\\nCorrelation Analysis:\\n  Check for relationships between features and the target variable. For example:\\n    Features with high correlation to the target may be predictive.\\n    High multicollinearity (correlation between features) can cause instability in some models.\\n\\nFeature Selection:\\n  Determine which features are likely to be useful for the model and consider dropping irrelevant or redundant features.\\n\\n5. Choose Appropriate Preprocessing\\nScaling or Normalization:\\n  Identify features with widely different ranges that need scaling (e.g., height in cm vs. weight in kg).\\n\\nEncoding:\\n  Understand categorical features and decide on encoding techniques like One-Hot Encoding or Label Encoding.\\n\\n6. Detect Data Imbalances\\nClass Imbalance in Classification:\\n  Check whether one class significantly outnumbers others.\\n  Example: Fraud detection datasets often have an imbalance where fraudulent cases are rare.\\n  Plan to address imbalance using techniques like oversampling, undersampling, or weighted loss functions.\\n\\n7. Understand Data Distributions\\nIdentify Transformations:\\n  Skewed data distributions can negatively affect model performance. EDA helps identify whether log transformations, Box-Cox, or other methods are \\n  needed to make data more normal-like.\\n\\n8. Ensure Data Quality\\nRemove Noise:\\n  Detect and handle noisy or irrelevant data.\\n\\nIdentify Incorrect Data:\\n  Spot data entry errors or inconsistencies (e.g., negative values for age).\\n\\n9. Select Suitable Models\\nAlgorithm Selection:\\n  Insights gained during EDA guide model selection. For example:\\n    If features are highly correlated, consider models like Ridge or Lasso regression.\\n    If data has complex interactions, tree-based models like Random Forest or Gradient Boosting may work better.\\n\\n10. Save Time and Effort\\nBy catching and addressing issues early, EDA prevents wasted time training models on poor-quality data or with suboptimal features.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q.11] Why do we have to perform EDA before fitting a model to the data?\n",
    "Ans. Performing Exploratory Data Analysis (EDA) before fitting a machine learning model is essential because it helps you understand the dataset, \n",
    "identify potential issues, and make informed decisions about preprocessing, feature engineering, and model selection. Here's why EDA is critical:\n",
    "\n",
    "1. Understand the Data\n",
    "Insight into Features:\n",
    "    EDA provides an overview of the dataset, such as the number of features, types of variables (categorical or numerical), and distributions.\n",
    "\n",
    "It answers questions like:\n",
    "  Are the features continuous, categorical, or binary?\n",
    "  What is the range of the values?\n",
    "\n",
    "Target Variable:\n",
    "  For supervised learning, understanding the target variable is crucial. For example:\n",
    "    Is the target variable balanced?\n",
    "    Does it have outliers?\n",
    "\n",
    "2. Detect Missing Values\n",
    "Identify Missing Data:\n",
    "  Missing values can lead to errors or biased model training. EDA helps identify the presence and extent of missing data.\n",
    "  Example: If 80% of a feature's values are missing, it might be better to drop that feature.\n",
    "\n",
    "Plan Imputation:\n",
    "  Decide whether to impute missing values (e.g., mean, median) or use advanced techniques like KNNImputer.\n",
    "\n",
    "3. Spot Outliers\n",
    "Outlier Detection:\n",
    "  EDA helps identify extreme values that could skew the model.\n",
    "  Example: In a regression model, large outliers in the target variable might dominate the loss function.\n",
    "\n",
    "Handle Appropriately:\n",
    "  Decide whether to remove, cap, or transform outliers.\n",
    "\n",
    "4. Explore Relationships Between Variables\n",
    "Correlation Analysis:\n",
    "  Check for relationships between features and the target variable. For example:\n",
    "    Features with high correlation to the target may be predictive.\n",
    "    High multicollinearity (correlation between features) can cause instability in some models.\n",
    "\n",
    "Feature Selection:\n",
    "  Determine which features are likely to be useful for the model and consider dropping irrelevant or redundant features.\n",
    "\n",
    "5. Choose Appropriate Preprocessing\n",
    "Scaling or Normalization:\n",
    "  Identify features with widely different ranges that need scaling (e.g., height in cm vs. weight in kg).\n",
    "\n",
    "Encoding:\n",
    "  Understand categorical features and decide on encoding techniques like One-Hot Encoding or Label Encoding.\n",
    "\n",
    "6. Detect Data Imbalances\n",
    "Class Imbalance in Classification:\n",
    "  Check whether one class significantly outnumbers others.\n",
    "  Example: Fraud detection datasets often have an imbalance where fraudulent cases are rare.\n",
    "  Plan to address imbalance using techniques like oversampling, undersampling, or weighted loss functions.\n",
    "\n",
    "7. Understand Data Distributions\n",
    "Identify Transformations:\n",
    "  Skewed data distributions can negatively affect model performance. EDA helps identify whether log transformations, Box-Cox, or other methods are \n",
    "  needed to make data more normal-like.\n",
    "\n",
    "8. Ensure Data Quality\n",
    "Remove Noise:\n",
    "  Detect and handle noisy or irrelevant data.\n",
    "\n",
    "Identify Incorrect Data:\n",
    "  Spot data entry errors or inconsistencies (e.g., negative values for age).\n",
    "\n",
    "9. Select Suitable Models\n",
    "Algorithm Selection:\n",
    "  Insights gained during EDA guide model selection. For example:\n",
    "    If features are highly correlated, consider models like Ridge or Lasso regression.\n",
    "    If data has complex interactions, tree-based models like Random Forest or Gradient Boosting may work better.\n",
    "\n",
    "10. Save Time and Effort\n",
    "By catching and addressing issues early, EDA prevents wasted time training models on poor-quality data or with suboptimal features.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec9538f9-7266-4b7b-b425-9355e0e3455e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q.12] What is correlation?\\nAns. \\nWhat is Correlation?\\nCorrelation is a statistical measure that describes the degree to which two variables are related or move together. It quantifies the strength and \\ndirection of the relationship between two variables.\\n\\n  Positive Correlation: When one variable increases, the other also increases (e.g., height and weight).\\n  Negative Correlation: When one variable increases, the other decreases (e.g., speed and travel time).\\n  No Correlation: No discernible relationship between the variables.\\n\\nKey Features of Correlation:\\n\\nRange : Correlation values range between -1 and 1:\\n  +1: Perfect positive correlation.\\n  -1: Perfect negative correlation.\\n   0: No correlation.\\n\\nDirection:\\n  Positive: Both variables increase or decrease together.\\n  Negative: One variable increases while the other decreases.\\n\\nMagnitude:\\n  The closer the correlation value is to 1 or -1, the stronger the relationship.\\n  A value near 0 indicates a weak or no relationship.\\n\\nLinear Relationship : Correlation assumes a linear relationship between the variables. Nonlinear relationships are not captured effectively.\\n\\nTypes of Correlation\\n\\n1. Pearson Correlation Coefficient : Measures the linear relationship between two variables.\\n\\n    Formula:\\n                𝑟 = Cov(𝑋,𝑌)/(𝜎𝑋.𝜎𝑌)\\n\\n    Where:\\n        Cov(𝑋,𝑌) : Covariance between\\n        𝜎𝑋, 𝜎𝑌 : Standard deviations of 𝑋 and 𝑌\\n\\n2. Spearman Rank Correlation:\\n\\n  Measures the strength and direction of a monotonic relationship using rank values of the data.\\n  Useful for non-linear relationships.\\n\\n3. Kendall’s Tau:\\n\\n  Measures the ordinal association between two variables, based on the concordance of data pairs.\\n\\nWhy is Correlation Important?\\n  Feature Selection:\\n    Helps identify relationships between features and target variables.\\n    Avoids multicollinearity by identifying highly correlated features.\\n\\n  Data Insights:\\n    Provides an initial understanding of how variables interact.\\n  \\n  Predictive Power:\\n    Correlated variables often have predictive significance in machine learning models.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q.12] What is correlation?\n",
    "Ans. \n",
    "What is Correlation?\n",
    "Correlation is a statistical measure that describes the degree to which two variables are related or move together. It quantifies the strength and \n",
    "direction of the relationship between two variables.\n",
    "\n",
    "  Positive Correlation: When one variable increases, the other also increases (e.g., height and weight).\n",
    "  Negative Correlation: When one variable increases, the other decreases (e.g., speed and travel time).\n",
    "  No Correlation: No discernible relationship between the variables.\n",
    "\n",
    "Key Features of Correlation:\n",
    "\n",
    "Range : Correlation values range between -1 and 1:\n",
    "  +1: Perfect positive correlation.\n",
    "  -1: Perfect negative correlation.\n",
    "   0: No correlation.\n",
    "\n",
    "Direction:\n",
    "  Positive: Both variables increase or decrease together.\n",
    "  Negative: One variable increases while the other decreases.\n",
    "\n",
    "Magnitude:\n",
    "  The closer the correlation value is to 1 or -1, the stronger the relationship.\n",
    "  A value near 0 indicates a weak or no relationship.\n",
    "\n",
    "Linear Relationship : Correlation assumes a linear relationship between the variables. Nonlinear relationships are not captured effectively.\n",
    "\n",
    "Types of Correlation\n",
    "\n",
    "1. Pearson Correlation Coefficient : Measures the linear relationship between two variables.\n",
    "\n",
    "    Formula:\n",
    "                𝑟 = Cov(𝑋,𝑌)/(𝜎𝑋.𝜎𝑌)\n",
    "\n",
    "    Where:\n",
    "        Cov(𝑋,𝑌) : Covariance between\n",
    "        𝜎𝑋, 𝜎𝑌 : Standard deviations of 𝑋 and 𝑌\n",
    "\n",
    "2. Spearman Rank Correlation:\n",
    "\n",
    "  Measures the strength and direction of a monotonic relationship using rank values of the data.\n",
    "  Useful for non-linear relationships.\n",
    "\n",
    "3. Kendall’s Tau:\n",
    "\n",
    "  Measures the ordinal association between two variables, based on the concordance of data pairs.\n",
    "\n",
    "Why is Correlation Important?\n",
    "  Feature Selection:\n",
    "    Helps identify relationships between features and target variables.\n",
    "    Avoids multicollinearity by identifying highly correlated features.\n",
    "\n",
    "  Data Insights:\n",
    "    Provides an initial understanding of how variables interact.\n",
    "  \n",
    "  Predictive Power:\n",
    "    Correlated variables often have predictive significance in machine learning models.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afa4548f-d561-4b28-aba6-be30c972e93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Q.13] What does negative correlation mean?\n",
    "Ans. Negative correlation describes a relationship between two variables in which one variable increases as the other decreases. This inverse \n",
    "relationship implies that when one variable goes up, the other tends to go down, and vice versa.\n",
    "\n",
    "Characteristics of Negative Correlation\n",
    "Correlation Coefficient:\n",
    "\n",
    "The correlation coefficient (𝑟) ranges from 0 to -1 for negative correlations.\n",
    "\n",
    "𝑟 = −1: Perfect negative correlation (variables move exactly in opposite directions).\n",
    "𝑟 = 0: No correlation.\n",
    "\n",
    "Direction:\n",
    "    A negative value of 𝑟 indicates an inverse relationship between the two variables.\n",
    "\n",
    "Magnitude:\n",
    "    The closer 𝑟 is to -1, the stronger the negative correlation.\n",
    "\n",
    "Visual Representation of Negative Correlation\n",
    "A scatterplot is often used to visualize negative correlation. The data points form a downward-sloping pattern, indicating that as one variable \n",
    "increases, the other decreases.\n",
    "\n",
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulated data with negative correlation\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = -2 * x + np.random.normal(0, 2, 100)  # Negative correlation with some noise\n",
    "\n",
    "# Scatterplot\n",
    "plt.scatter(x, y)\n",
    "plt.title('Scatterplot of Negative Correlation')\n",
    "plt.xlabel('Variable X')\n",
    "plt.ylabel('Variable Y')\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "Why Negative Correlation is Important\n",
    "\n",
    "Understanding Relationships:\n",
    "    Identifying negative correlations helps explain how changes in one variable affect another.\n",
    "\n",
    "Feature Selection:\n",
    "    In machine learning, negatively correlated features with the target variable can still be useful for prediction.\n",
    "\n",
    "Business and Strategy:\n",
    "    Recognizing inverse relationships aids in decision-making, such as pricing strategies or cost management.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "203631af-83ed-4b03-8270-1bb1331b0ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing default(Pearson Correlation) using Pandas\n",
      "     X    Y    Z\n",
      "X  1.0 -1.0  1.0\n",
      "Y -1.0  1.0 -1.0\n",
      "Z  1.0 -1.0  1.0\n",
      " \n",
      "Printing Spearman Correlation using Pandas\n",
      "     X    Y    Z\n",
      "X  1.0 -1.0  1.0\n",
      "Y -1.0  1.0 -1.0\n",
      "Z  1.0 -1.0  1.0\n",
      " \n",
      "Printing Pearson Correlation Coeff. using numpy\n",
      "[[ 1. -1.]\n",
      " [-1.  1.]]\n",
      " \n",
      "Printing Pearson Correlation using Scipy.stats Module\n",
      "Pearson Correlation: -1.0\n",
      "P-value: 0.0\n",
      " \n",
      "Printing Spearman Correlation using Scipy.stats Module\n",
      "Spearman Correlation: -0.9999999999999999\n",
      "P-value: 1.4042654220543672e-24\n",
      " \n",
      "Printing Kendall's Tau using Scipy.stats Module\n",
      "Kendall Tau Correlation: -0.9999999999999999\n",
      "P-value: 0.016666666666666666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'When to Use Which Method?\\nPearson: When the relationship is linear, and both variables are continuous.\\nSpearman/Kendall: For nonlinear relationships or ordinal data.\\nHeatmaps: To analyze correlations across multiple variables.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q.14] How can you find correlation between variables in Python?\n",
    "Ans. In Python, you can calculate the correlation between variables using statistical libraries like pandas, numpy, or scipy. Here's how you can do it:\n",
    "\n",
    "\n",
    "In Python, you can calculate the correlation between variables using statistical libraries like pandas, numpy, or scipy. Here's how you can do it:\n",
    "\n",
    "1. Using Pandas\n",
    "Pearson Correlation (Default Method)\n",
    "The pandas.DataFrame.corr() method computes the correlation matrix.'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Example dataset\n",
    "data = pd.DataFrame({\n",
    "    'X': [1, 2, 3, 4, 5],\n",
    "    'Y': [5, 4, 3, 2, 1],\n",
    "    'Z': [2, 3, 4, 5, 6]\n",
    "})\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = data.corr() #this will use Pearson Correlation(Default Method)\n",
    "print(\"Printing default(Pearson Correlation) using Pandas\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "'''\n",
    "Specify Correlation Method\n",
    "You can use different methods:\n",
    "\n",
    "method='pearson': Default. Measures linear correlation.\n",
    "method='spearman': Measures rank correlation (nonlinear relationships).\n",
    "method='kendall': Measures ordinal association.\n",
    "'''\n",
    "print(\" \")\n",
    "print(\"Printing Spearman Correlation using Pandas\")\n",
    "# Spearman correlation\n",
    "spearman_corr = data.corr(method='spearman')\n",
    "print(spearman_corr)\n",
    "print(\" \")\n",
    "\n",
    "'''\n",
    "2. Using Numpy\n",
    "The numpy.corrcoef() function calculates the Pearson correlation coefficient.'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([5, 4, 3, 2, 1])\n",
    "\n",
    "# Compute correlation\n",
    "correlation = np.corrcoef(x, y)\n",
    "print(\"Printing Pearson Correlation Coeff. using numpy\")\n",
    "print(correlation)\n",
    "print(\" \")\n",
    "\n",
    "'''3. Using Scipy\n",
    "The scipy.stats module provides functions for calculating specific correlations.'''\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Example data\n",
    "x = [1, 2, 3, 4, 5]\n",
    "y = [5, 4, 3, 2, 1]\n",
    "\n",
    "# Compute Pearson correlation and p-value\n",
    "corr, p_value = pearsonr(x, y)\n",
    "print(\"Printing Pearson Correlation using Scipy.stats Module\")\n",
    "print(\"Pearson Correlation:\", corr)\n",
    "print(\"P-value:\", p_value)\n",
    "print(\" \")\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Compute Spearman correlation and p-value\n",
    "corr, p_value = spearmanr(x, y)\n",
    "print(\"Printing Spearman Correlation using Scipy.stats Module\")\n",
    "print(\"Spearman Correlation:\", corr)\n",
    "print(\"P-value:\", p_value)\n",
    "print(\" \")\n",
    "\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "# Compute Kendall correlation and p-value\n",
    "corr, p_value = kendalltau(x, y)\n",
    "print(\"Printing Kendall's Tau using Scipy.stats Module\")\n",
    "print(\"Kendall Tau Correlation:\", corr)\n",
    "print(\"P-value:\", p_value)\n",
    "\n",
    "'''When to Use Which Method?\n",
    "Pearson: When the relationship is linear, and both variables are continuous.\n",
    "Spearman/Kendall: For nonlinear relationships or ordinal data.\n",
    "Heatmaps: To analyze correlations across multiple variables.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b465bcf6-53ad-4ab8-b2d4-86e53bb97e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q.15] What is causation? Explain difference between correlation and causation with an example.\\nAns. Causation means that a change in one variable directly causes a change in another variable. It implies a cause-and-effect relationship between \\ntwo variables. If 𝑋 causes 𝑌, then changes in 𝑋 will produce predictable changes in 𝑌.\\n\\nFor example:\\n    Smoking causes an increased risk of lung cancer. Here, smoking is the cause, and lung cancer is the effect.\\n\\nAspect\\t                         Correlation\\t                                                        Causation\\n\\nDefinition      \\tA statistical relationship between two variables.\\t            A cause-and-effect relationship between variables.\\n\\nDirectionality    \\tDoes not imply direction or causation.                      \\tImplies direction (cause leads to effect).\\n\\nMechanism\\t        No guarantee of a direct link or interaction.\\t                Requires evidence of a direct link or mechanism.\\n\\nConfounding\\t        May occur due to a third variable influencing both.\\t            Confounding must be ruled out to establish causation.\\n\\nKey Insights\\n\\nCorrelation without Causation:\\n    Correlation does not always imply causation. Just because two variables move together does not mean one causes the other.\\n    Example: Shoe size and reading ability in children are correlated, but larger shoe size does not cause better reading skills. The actual cause \\n    is age—older children have larger feet and better reading skills.\\n\\nEstablishing Causation:\\n    To determine causation, we need:\\n    Temporal Relationship: The cause must occur before the effect.\\n    Controlled Experiments: Evidence from experiments where confounding factors are controlled.\\n    Mechanistic Evidence: A plausible explanation for how one variable causes the other.\\n\\nHow to Differentiate Correlation from Causation?\\n\\nUse Experiments:\\nRandomized controlled trials (RCTs) are the gold standard for establishing causation.\\n\\nControl for Confounders:\\nStatistical techniques like regression models or propensity score matching can help account for confounding variables.\\n\\nAnalyze Directionality:\\nEstablish whether one variable consistently precedes the other.\\n\\nGranger Causality:\\nIn time-series data, this test helps determine whether one time series predicts changes in another.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q.15] What is causation? Explain difference between correlation and causation with an example.\n",
    "Ans. Causation means that a change in one variable directly causes a change in another variable. It implies a cause-and-effect relationship between \n",
    "two variables. If 𝑋 causes 𝑌, then changes in 𝑋 will produce predictable changes in 𝑌.\n",
    "\n",
    "For example:\n",
    "    Smoking causes an increased risk of lung cancer. Here, smoking is the cause, and lung cancer is the effect.\n",
    "\n",
    "Aspect\t                         Correlation\t                                                        Causation\n",
    "\n",
    "Definition      \tA statistical relationship between two variables.\t            A cause-and-effect relationship between variables.\n",
    "\n",
    "Directionality    \tDoes not imply direction or causation.                      \tImplies direction (cause leads to effect).\n",
    "\n",
    "Mechanism\t        No guarantee of a direct link or interaction.\t                Requires evidence of a direct link or mechanism.\n",
    "\n",
    "Confounding\t        May occur due to a third variable influencing both.\t            Confounding must be ruled out to establish causation.\n",
    "\n",
    "Key Insights\n",
    "\n",
    "Correlation without Causation:\n",
    "    Correlation does not always imply causation. Just because two variables move together does not mean one causes the other.\n",
    "    Example: Shoe size and reading ability in children are correlated, but larger shoe size does not cause better reading skills. The actual cause \n",
    "    is age—older children have larger feet and better reading skills.\n",
    "\n",
    "Establishing Causation:\n",
    "    To determine causation, we need:\n",
    "    Temporal Relationship: The cause must occur before the effect.\n",
    "    Controlled Experiments: Evidence from experiments where confounding factors are controlled.\n",
    "    Mechanistic Evidence: A plausible explanation for how one variable causes the other.\n",
    "\n",
    "How to Differentiate Correlation from Causation?\n",
    "\n",
    "Use Experiments:\n",
    "Randomized controlled trials (RCTs) are the gold standard for establishing causation.\n",
    "\n",
    "Control for Confounders:\n",
    "Statistical techniques like regression models or propensity score matching can help account for confounding variables.\n",
    "\n",
    "Analyze Directionality:\n",
    "Establish whether one variable consistently precedes the other.\n",
    "\n",
    "Granger Causality:\n",
    "In time-series data, this test helps determine whether one time series predicts changes in another.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faaef775-906b-42ce-ada6-380ff76b81d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights: [0.95 2.03]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2. Momentum\\nMomentum builds upon SGD by adding a fraction of the previous update to the current one, helping to overcome oscillations and converge faster.\\n\\nFormula:\\n𝑣𝑡 = 𝛾.𝑣𝑡−1 + 𝜂.∇𝐿\\n\\n𝜃𝑡 = 𝜃𝑡−1 − 𝑣𝑡\\n \\nWhere:\\n\\n𝑣𝑡 : Velocity (momentum term).\\n𝛾: Momentum coefficient (e.g., 0.9).\\n\\n3. Adagrad (Adaptive Gradient Algorithm)\\nAdagrad adapts the learning rate for each parameter based on the cumulative sum of past squared gradients. It is effective for sparse data.\\n\\n4. RMSProp (Root Mean Square Propagation)\\nRMSProp is a refinement of Adagrad that decays the cumulative gradient sum over time, avoiding overly small learning rates.\\n\\n5. Adam (Adaptive Moment Estimation)\\nAdam combines Momentum and RMSProp, maintaining running averages of both the gradients and their squared values.\\n\\n6. Adadelta\\nAdadelta builds on Adagrad by restricting the sum of squared gradients to a fixed window, improving performance on non-stationary objectives.\\n\\n7. Nesterov Accelerated Gradient (NAG)\\nNAG improves Momentum by looking ahead at the next step before calculating the gradient.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q.16] What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
    "Ans. An optimizer is an algorithm used to adjust the weights and biases of a machine learning model to minimize the loss function during training. \n",
    "It plays a crucial role in how a model learns from data by iteratively updating parameters in the direction that reduces the error.\n",
    "\n",
    "Role of an Optimizer\n",
    "  Minimizing Loss: Reduces the difference between predicted and actual values.\n",
    "  Adjusting Weights: Updates model parameters based on gradients calculated from the loss function.\n",
    "  Controlling Learning Rate: Determines the step size in the optimization process.\n",
    "\n",
    "Types of Optimizers\n",
    "  Optimizers are broadly classified into:\n",
    "\n",
    "  1. First-Order Optimization Algorithms:\n",
    "\n",
    "     Use gradients of the loss function for updates.\n",
    "     Examples: Gradient Descent, Stochastic Gradient Descent (SGD).\n",
    "\n",
    "  2. Advanced Optimization Algorithms:\n",
    "\n",
    "     Improve upon basic gradient descent by adapting learning rates or using momentum.\n",
    "     Examples: Momentum, RMSProp, Adam, Adagrad, Adadelta.\n",
    "\n",
    "1. Gradient Descent\n",
    "Gradient Descent calculates the gradient of the loss function with respect to the model’s parameters and updates them iteratively.\n",
    "\n",
    "Types of Gradient Descent:\n",
    "\n",
    " 1. Batch Gradient Descent:\n",
    "\n",
    "    Uses the entire dataset to compute gradients.\n",
    "    Slow but accurate.\n",
    "\n",
    " 2. Stochastic Gradient Descent (SGD):\n",
    "\n",
    "    Uses one data point per iteration.\n",
    "    Faster but noisier updates.\n",
    "\n",
    " 3. Mini-Batch Gradient Descent:\n",
    "\n",
    "    Uses a subset (mini-batch) of the dataset per iteration.\n",
    "    Balances speed and stability.'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Example: Single update step in SGD\n",
    "weights = np.array([1.0, 2.0])  # Initial weights\n",
    "learning_rate = 0.1\n",
    "gradient = np.array([0.5, -0.3])  # Gradient of the loss function\n",
    "\n",
    "# Update weights\n",
    "weights = weights - learning_rate * gradient\n",
    "print(\"Updated weights:\", weights)\n",
    "\n",
    "'''2. Momentum\n",
    "Momentum builds upon SGD by adding a fraction of the previous update to the current one, helping to overcome oscillations and converge faster.\n",
    "\n",
    "Formula:\n",
    "𝑣𝑡 = 𝛾.𝑣𝑡−1 + 𝜂.∇𝐿\n",
    "\n",
    "𝜃𝑡 = 𝜃𝑡−1 − 𝑣𝑡\n",
    " \n",
    "Where:\n",
    "\n",
    "𝑣𝑡 : Velocity (momentum term).\n",
    "𝛾: Momentum coefficient (e.g., 0.9).\n",
    "\n",
    "3. Adagrad (Adaptive Gradient Algorithm)\n",
    "Adagrad adapts the learning rate for each parameter based on the cumulative sum of past squared gradients. It is effective for sparse data.\n",
    "\n",
    "4. RMSProp (Root Mean Square Propagation)\n",
    "RMSProp is a refinement of Adagrad that decays the cumulative gradient sum over time, avoiding overly small learning rates.\n",
    "\n",
    "5. Adam (Adaptive Moment Estimation)\n",
    "Adam combines Momentum and RMSProp, maintaining running averages of both the gradients and their squared values.\n",
    "\n",
    "6. Adadelta\n",
    "Adadelta builds on Adagrad by restricting the sum of squared gradients to a fixed window, improving performance on non-stationary objectives.\n",
    "\n",
    "7. Nesterov Accelerated Gradient (NAG)\n",
    "NAG improves Momentum by looking ahead at the next step before calculating the gradient.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b75b7fc-17b7-4000-b5ea-f50d095d2a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q.17] What is sklearn.linear_model ?\\nAns. sklearn.linear_model is a module in the scikit-learn library that contains various linear models and related tools for regression and \\nclassification tasks. These models assume a linear relationship between the input features (𝑋) and the target variable (𝑦).\\n\\nFeatures of sklearn.linear_model\\n\\n 1. Variety of Models : Includes models like Linear Regression, Logistic Regression, Ridge, Lasso, ElasticNet, and more.\\n\\n 2. Regularization : Supports regularization techniques (e.g., L1, L2) to prevent overfitting.\\n\\n 3. Flexibility : Can handle simple tasks like ordinary least squares regression to more complex ones like logistic regression for classification.\\n\\n 4. Ease of Use : Provides a consistent API for training and prediction.\\n\\nCommon Models in sklearn.linear_model\\n\\n1. Linear Regression\\nUsed for: Predicting continuous values assuming a linear relationship.\\nAlgorithm: Minimizes the residual sum of squares between observed and predicted values.\\n\\n2. Logistic Regression\\nUsed for: Classification tasks (binary or multi-class).\\nAlgorithm: Fits a logistic function to the data and predicts probabilities.\\n\\n3. Ridge Regression\\nUsed for: Regularized linear regression (L2 regularization).\\nPurpose: Reduces overfitting by penalizing large coefficients.\\n\\n4. Lasso Regression\\nUsed for: Regularized linear regression (L1 regularization).\\nPurpose: Shrinks some coefficients to zero, effectively performing feature selection.\\n\\n5. ElasticNet\\nUsed for: Regularized linear regression combining L1 and L2 penalties.\\nPurpose: Balances feature selection and coefficient shrinkage.\\n\\n6. Perceptron\\nUsed for: Linear binary classification.\\nPurpose: Implements a simple perceptron learning algorithm.\\n\\n7. SGDClassifier and SGDRegressor\\nUsed for: Stochastic Gradient Descent-based linear models.\\nPurpose: Efficient for large-scale problems.\\n\\nAdditional Models\\n1. BayesianRidge: Bayesian approach to linear regression.\\n2. HuberRegressor: Robust regression to handle outliers.\\n3. PassiveAggressiveClassifier/Regressor: Online learning algorithms for classification and regression.\\n\\nChoosing the Right Model\\nFor Regression:\\n\\n    Simple: LinearRegression\\n    Regularized: Ridge, Lasso, ElasticNet\\n    Robust to outliers: HuberRegressor\\n\\nFor Classification:\\n\\n    Binary/Multi-class: LogisticRegression\\n    Large datasets: SGDClassifier\\n    Online learning: Perceptron\\n\\nAdvantages of sklearn.linear_model\\n    Easy to implement and integrate into pipelines.\\n    Efficient for problems where the relationship between features and target is linear.\\n    Provides built-in regularization techniques.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q.17] What is sklearn.linear_model ?\n",
    "Ans. sklearn.linear_model is a module in the scikit-learn library that contains various linear models and related tools for regression and \n",
    "classification tasks. These models assume a linear relationship between the input features (𝑋) and the target variable (𝑦).\n",
    "\n",
    "Features of sklearn.linear_model\n",
    "\n",
    " 1. Variety of Models : Includes models like Linear Regression, Logistic Regression, Ridge, Lasso, ElasticNet, and more.\n",
    "\n",
    " 2. Regularization : Supports regularization techniques (e.g., L1, L2) to prevent overfitting.\n",
    "\n",
    " 3. Flexibility : Can handle simple tasks like ordinary least squares regression to more complex ones like logistic regression for classification.\n",
    "\n",
    " 4. Ease of Use : Provides a consistent API for training and prediction.\n",
    "\n",
    "Common Models in sklearn.linear_model\n",
    "\n",
    "1. Linear Regression\n",
    "Used for: Predicting continuous values assuming a linear relationship.\n",
    "Algorithm: Minimizes the residual sum of squares between observed and predicted values.\n",
    "\n",
    "2. Logistic Regression\n",
    "Used for: Classification tasks (binary or multi-class).\n",
    "Algorithm: Fits a logistic function to the data and predicts probabilities.\n",
    "\n",
    "3. Ridge Regression\n",
    "Used for: Regularized linear regression (L2 regularization).\n",
    "Purpose: Reduces overfitting by penalizing large coefficients.\n",
    "\n",
    "4. Lasso Regression\n",
    "Used for: Regularized linear regression (L1 regularization).\n",
    "Purpose: Shrinks some coefficients to zero, effectively performing feature selection.\n",
    "\n",
    "5. ElasticNet\n",
    "Used for: Regularized linear regression combining L1 and L2 penalties.\n",
    "Purpose: Balances feature selection and coefficient shrinkage.\n",
    "\n",
    "6. Perceptron\n",
    "Used for: Linear binary classification.\n",
    "Purpose: Implements a simple perceptron learning algorithm.\n",
    "\n",
    "7. SGDClassifier and SGDRegressor\n",
    "Used for: Stochastic Gradient Descent-based linear models.\n",
    "Purpose: Efficient for large-scale problems.\n",
    "\n",
    "Additional Models\n",
    "1. BayesianRidge: Bayesian approach to linear regression.\n",
    "2. HuberRegressor: Robust regression to handle outliers.\n",
    "3. PassiveAggressiveClassifier/Regressor: Online learning algorithms for classification and regression.\n",
    "\n",
    "Choosing the Right Model\n",
    "For Regression:\n",
    "\n",
    "    Simple: LinearRegression\n",
    "    Regularized: Ridge, Lasso, ElasticNet\n",
    "    Robust to outliers: HuberRegressor\n",
    "\n",
    "For Classification:\n",
    "\n",
    "    Binary/Multi-class: LogisticRegression\n",
    "    Large datasets: SGDClassifier\n",
    "    Online learning: Perceptron\n",
    "\n",
    "Advantages of sklearn.linear_model\n",
    "    Easy to implement and integrate into pipelines.\n",
    "    Efficient for problems where the relationship between features and target is linear.\n",
    "    Provides built-in regularization techniques.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd40fe11-83ee-4b9d-b436-ce9d96abf482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Q.18] What does model.fit() do? What arguments must be given?\\nAns. In scikit-learn, the fit() method is used to train a machine learning model. It takes the training data and learns the relationship between the \\ninput features (𝑋) and the target variable (𝑦) by adjusting the model's parameters.\\n\\nSteps Performed by fit()\\n\\n  # Initialization: Prepares the model for training by initializing necessary structures, like weights and biases for linear models.\\n  # Optimization:\\n    Calculates the loss function based on the predictions and actual target values.\\n    Uses an optimization algorithm (e.g., Gradient Descent) to minimize the loss and adjust model parameters.\\n  # Model Training: Stores the learned parameters to be used for prediction later with methods like predict().\\n\\nArguments for model.fit()\\nThe exact arguments depend on the type of model, but typically, the following are required:\\n\\nRequired Arguments:\\n\\n𝑋 : Features or input data (training data).\\n    Format : 2D array-like structure (e.g., DataFrame or NumPy array) of shape (𝑛 samples, 𝑛 features).\\n𝑦 : Target labels or dependent variable.\\n    Format: 1D array-like structure for regression or classification problems.\\n\\nOptional Arguments:\\n\\nsample_weight: Weights for each sample, allowing you to give more importance to certain samples during training.\\nclasses: For certain classification models, you might need to specify the unique class labels.\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q.18] What does model.fit() do? What arguments must be given?\n",
    "Ans. In scikit-learn, the fit() method is used to train a machine learning model. It takes the training data and learns the relationship between the \n",
    "input features (𝑋) and the target variable (𝑦) by adjusting the model's parameters.\n",
    "\n",
    "Steps Performed by fit()\n",
    "\n",
    "  # Initialization: Prepares the model for training by initializing necessary structures, like weights and biases for linear models.\n",
    "  # Optimization:\n",
    "    Calculates the loss function based on the predictions and actual target values.\n",
    "    Uses an optimization algorithm (e.g., Gradient Descent) to minimize the loss and adjust model parameters.\n",
    "  # Model Training: Stores the learned parameters to be used for prediction later with methods like predict().\n",
    "\n",
    "Arguments for model.fit()\n",
    "The exact arguments depend on the type of model, but typically, the following are required:\n",
    "\n",
    "Required Arguments:\n",
    "\n",
    "𝑋 : Features or input data (training data).\n",
    "    Format : 2D array-like structure (e.g., DataFrame or NumPy array) of shape (𝑛 samples, 𝑛 features).\n",
    "𝑦 : Target labels or dependent variable.\n",
    "    Format: 1D array-like structure for regression or classification problems.\n",
    "\n",
    "Optional Arguments:\n",
    "\n",
    "sample_weight: Weights for each sample, allowing you to give more importance to certain samples during training.\n",
    "classes: For certain classification models, you might need to specify the unique class labels.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f517f858-7747-4dbc-852b-d8d045baf20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q.19] What does model.predict() do? What arguments must be given?\\nAns. The predict() method in scikit-learn is used to make predictions on new, unseen data after a model has been trained using the fit() method. \\nIt takes the input features (𝑋) and applies the learned parameters of the model to generate predictions.\\n\\nSteps Performed by predict()\\n  Input Validation:\\n\\n    Ensures that the input data (𝑋) is in the correct shape and format.\\n    Checks for compatibility with the features used during training.\\n\\n  Forward Computation:\\n\\n    Applies the learned relationship (from fit()) to the input data.\\n    Computes the predicted values (𝑦pred) based on the model type:\\n      Regression: Returns continuous values.\\n      Classification: Returns class labels.\\n\\nArguments for model.predict()\\nRequired Argument:\\n\\n𝑋test : Input features for which predictions are to be made.\\n    Shape: (𝑛samples, 𝑛features)\\n    Must match the number of features used during training.\\n\\nOptional Arguments:\\n\\n    Most models do not require optional arguments for predict().\\n    Some specialized models may allow extra parameters (e.g., thresholds for classification).'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q.19] What does model.predict() do? What arguments must be given?\n",
    "Ans. The predict() method in scikit-learn is used to make predictions on new, unseen data after a model has been trained using the fit() method. \n",
    "It takes the input features (𝑋) and applies the learned parameters of the model to generate predictions.\n",
    "\n",
    "Steps Performed by predict()\n",
    "  Input Validation:\n",
    "\n",
    "    Ensures that the input data (𝑋) is in the correct shape and format.\n",
    "    Checks for compatibility with the features used during training.\n",
    "\n",
    "  Forward Computation:\n",
    "\n",
    "    Applies the learned relationship (from fit()) to the input data.\n",
    "    Computes the predicted values (𝑦pred) based on the model type:\n",
    "      Regression: Returns continuous values.\n",
    "      Classification: Returns class labels.\n",
    "\n",
    "Arguments for model.predict()\n",
    "Required Argument:\n",
    "\n",
    "𝑋test : Input features for which predictions are to be made.\n",
    "    Shape: (𝑛samples, 𝑛features)\n",
    "    Must match the number of features used during training.\n",
    "\n",
    "Optional Arguments:\n",
    "\n",
    "    Most models do not require optional arguments for predict().\n",
    "    Some specialized models may allow extra parameters (e.g., thresholds for classification).'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eba484ec-96d7-48bb-9ac9-e5be8be731eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Q.19] What are continuous and categorical variables?\\nAns. In data analysis and machine learning, variables are typically classified into continuous and categorical types based on the type of data they hold.\\n\\n1. Continuous Variables\\nA continuous variable represents data that can take any numerical value within a range. These values are often measured and can have an infinite number \\nof possible values.\\n\\nCharacteristics:\\n    Numeric data with decimal or fractional values.\\n    Can be ordered and measured.\\n    Have meaningful intervals and ratios.\\n\\nExamples:\\n    Height (in centimeters, meters, etc.).\\n    Temperature (in Celsius, Fahrenheit, or Kelvin).\\n    Age (in years, months, etc.).\\n    Income (in dollars).\\n\\nIn Machine Learning:\\n    Typically used directly as features in models.\\n    Examples of algorithms handling continuous variables include regression models, decision trees, etc.\\n\\n2. Categorical Variables:\\nDefinition: Categorical variables, also called qualitative variables, represent categories or groups. These variables can take a limited, fixed number \\n            of possible values, known as categories or levels. They are usually used to represent types, groups, or labels rather than quantities.\\n\\nTypes of Categorical Variables:\\n\\n  Nominal Variables : These are categorical variables with no natural order or ranking between categories.\\n    Examples:\\n      Gender: Male, Female, Non-binary.\\n      Color: Red, Blue, Green.\\n      Country: USA, Canada, India, etc.\\n  Ordinal Variables : These are categorical variables with a natural order or ranking between categories.\\n    Examples:\\n      Education Level: High School, Bachelor's, Master's, PhD.\\n      Rating: Poor, Fair, Good, Excellent.\\n      Age Groups: 18-25, 26-35, 36-45, etc.\\n\\nProperties:\\n\\n  They represent labels or groups that are often non-numeric.\\n  The categories are discrete, and they do not have a meaningful numeric interpretation (except for ordinal variables, which have some inherent order).\\n  They can be encoded into numerical values for use in machine learning algorithms (e.g., one-hot encoding, label encoding).\\n\\nMathematical Representation: Categorical variables can be represented using labels (text) or encoded as numbers (1, 2, 3, etc.) for certain \\n                             algorithms that require numeric input.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q.20] What are continuous and categorical variables?\n",
    "Ans. In data analysis and machine learning, variables are typically classified into continuous and categorical types based on the type of data they hold.\n",
    "\n",
    "1. Continuous Variables\n",
    "A continuous variable represents data that can take any numerical value within a range. These values are often measured and can have an infinite number \n",
    "of possible values.\n",
    "\n",
    "Characteristics:\n",
    "    Numeric data with decimal or fractional values.\n",
    "    Can be ordered and measured.\n",
    "    Have meaningful intervals and ratios.\n",
    "\n",
    "Examples:\n",
    "    Height (in centimeters, meters, etc.).\n",
    "    Temperature (in Celsius, Fahrenheit, or Kelvin).\n",
    "    Age (in years, months, etc.).\n",
    "    Income (in dollars).\n",
    "\n",
    "In Machine Learning:\n",
    "    Typically used directly as features in models.\n",
    "    Examples of algorithms handling continuous variables include regression models, decision trees, etc.\n",
    "\n",
    "2. Categorical Variables:\n",
    "Definition: Categorical variables, also called qualitative variables, represent categories or groups. These variables can take a limited, fixed number \n",
    "            of possible values, known as categories or levels. They are usually used to represent types, groups, or labels rather than quantities.\n",
    "\n",
    "Types of Categorical Variables:\n",
    "\n",
    "  Nominal Variables : These are categorical variables with no natural order or ranking between categories.\n",
    "    Examples:\n",
    "      Gender: Male, Female, Non-binary.\n",
    "      Color: Red, Blue, Green.\n",
    "      Country: USA, Canada, India, etc.\n",
    "  Ordinal Variables : These are categorical variables with a natural order or ranking between categories.\n",
    "    Examples:\n",
    "      Education Level: High School, Bachelor's, Master's, PhD.\n",
    "      Rating: Poor, Fair, Good, Excellent.\n",
    "      Age Groups: 18-25, 26-35, 36-45, etc.\n",
    "\n",
    "Properties:\n",
    "\n",
    "  They represent labels or groups that are often non-numeric.\n",
    "  The categories are discrete, and they do not have a meaningful numeric interpretation (except for ordinal variables, which have some inherent order).\n",
    "  They can be encoded into numerical values for use in machine learning algorithms (e.g., one-hot encoding, label encoding).\n",
    "\n",
    "Mathematical Representation: Categorical variables can be represented using labels (text) or encoded as numbers (1, 2, 3, etc.) for certain \n",
    "                             algorithms that require numeric input.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81e4e188-940e-4e80-854e-0fac9d2dccf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q.21] What is feature scaling? How does it help in Machine Learning?\\nAns. Feature scaling is a preprocessing technique in machine learning used to normalize or standardize the range of independent variables or \\nfeatures in the dataset. It ensures that all features contribute equally to the model and prevents features with larger scales from dominating \\nthe learning process.\\n\\nImportance of Feature Scaling :\\n\\n 1. Prevents Bias in Models:\\n    Algorithms that rely on distance metrics (e.g., k-NN, SVM) are sensitive to the scale of features.\\n    Larger-scaled features can disproportionately affect the results.\\n\\n 2. Improves Convergence in Optimization:\\n    Algorithms like gradient descent converge faster when features are on a similar scale.\\n    Prevents the optimization algorithm from oscillating.\\n\\n 3. Maintains Consistency : Features measured on different scales (e.g., age in years vs. income in dollars) can distort results.\\n\\n 4. Enhances Interpretability : In some cases, scaling makes coefficients or feature importance easier to interpret.\\n\\nWhen Is Feature Scaling Necessary?\\n  Required for algorithms that depend on feature magnitudes:\\n\\n    Support Vector Machines (SVMs)\\n    k-Nearest Neighbors (k-NN)\\n    Principal Component Analysis (PCA)\\n    Gradient Descent-based algorithms (e.g., Logistic Regression, Neural Networks)\\n\\n  Not required for tree-based models:\\n    Decision Trees, Random Forests, Gradient Boosting Trees (e.g., XGBoost, LightGBM) are scale-invariant.\\n\\nHow Feature Scaling Helps in Machine Learning\\n\\n  Equal Contribution of Features:\\n      Ensures that all features contribute equally to distance-based algorithms like k-NN and SVM.\\n\\n  Improves Gradient Descent:\\n      Faster convergence by reducing the cost function oscillations.\\n\\n  Enhances PCA Performance:\\n      PCA seeks to maximize variance, so scaling ensures no feature dominates the principal components.\\n\\n  Avoids Dominance by Larger Features:\\n      Prevents bias in models where features with larger values dominate others.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q.21] What is feature scaling? How does it help in Machine Learning?\n",
    "Ans. Feature scaling is a preprocessing technique in machine learning used to normalize or standardize the range of independent variables or \n",
    "features in the dataset. It ensures that all features contribute equally to the model and prevents features with larger scales from dominating \n",
    "the learning process.\n",
    "\n",
    "Importance of Feature Scaling :\n",
    "\n",
    " 1. Prevents Bias in Models:\n",
    "    Algorithms that rely on distance metrics (e.g., k-NN, SVM) are sensitive to the scale of features.\n",
    "    Larger-scaled features can disproportionately affect the results.\n",
    "\n",
    " 2. Improves Convergence in Optimization:\n",
    "    Algorithms like gradient descent converge faster when features are on a similar scale.\n",
    "    Prevents the optimization algorithm from oscillating.\n",
    "\n",
    " 3. Maintains Consistency : Features measured on different scales (e.g., age in years vs. income in dollars) can distort results.\n",
    "\n",
    " 4. Enhances Interpretability : In some cases, scaling makes coefficients or feature importance easier to interpret.\n",
    "\n",
    "When Is Feature Scaling Necessary?\n",
    "  Required for algorithms that depend on feature magnitudes:\n",
    "\n",
    "    Support Vector Machines (SVMs)\n",
    "    k-Nearest Neighbors (k-NN)\n",
    "    Principal Component Analysis (PCA)\n",
    "    Gradient Descent-based algorithms (e.g., Logistic Regression, Neural Networks)\n",
    "\n",
    "  Not required for tree-based models:\n",
    "    Decision Trees, Random Forests, Gradient Boosting Trees (e.g., XGBoost, LightGBM) are scale-invariant.\n",
    "\n",
    "How Feature Scaling Helps in Machine Learning\n",
    "\n",
    "  Equal Contribution of Features:\n",
    "      Ensures that all features contribute equally to distance-based algorithms like k-NN and SVM.\n",
    "\n",
    "  Improves Gradient Descent:\n",
    "      Faster convergence by reducing the cost function oscillations.\n",
    "\n",
    "  Enhances PCA Performance:\n",
    "      PCA seeks to maximize variance, so scaling ensures no feature dominates the principal components.\n",
    "\n",
    "  Avoids Dominance by Larger Features:\n",
    "      Prevents bias in models where features with larger values dominate others.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "915e9940-2970-4bf3-8d47-d0501c0d8cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "   Age  Income\n",
      "0   25   50000\n",
      "1   30   60000\n",
      "2   35   55000\n",
      "3   40   45000\n",
      "\n",
      "Normalized Data:\n",
      "        Age    Income\n",
      "0  0.000000  0.333333\n",
      "1  0.333333  1.000000\n",
      "2  0.666667  0.666667\n",
      "3  1.000000  0.000000\n",
      "\n",
      "Standardized Data:\n",
      "        Age    Income\n",
      "0 -1.341641 -0.447214\n",
      "1 -0.447214  1.341641\n",
      "2  0.447214  0.447214\n",
      "3  1.341641 -1.341641\n",
      "\n",
      "Robust Scaled Data:\n",
      "        Age    Income\n",
      "0 -1.000000 -0.333333\n",
      "1 -0.333333  1.000000\n",
      "2  0.333333  0.333333\n",
      "3  1.000000 -1.000000\n",
      "\n",
      "Custom Min-Max Scaled Data:\n",
      "        Age    Income\n",
      "0  0.000000  0.333333\n",
      "1  0.333333  1.000000\n",
      "2  0.666667  0.666667\n",
      "3  1.000000  0.000000\n",
      "\n",
      "Scaled Training Data:\n",
      "[[ 1.06904497 -1.22474487]\n",
      " [-1.33630621  0.        ]\n",
      " [ 0.26726124  1.22474487]]\n",
      "\n",
      "Scaled Testing Data:\n",
      "[[-0.53452248  2.44948974]]\n"
     ]
    }
   ],
   "source": [
    "'''Q.22] How do we perform scaling in Python?\n",
    "Ans. In Python, scaling can be performed using libraries like scikit-learn, which provides robust tools for various scaling techniques such \n",
    "as normalization, standardization, and robust scaling. Here's how you can perform scaling step-by-step:\n",
    "\n",
    "1. Dataset Setup\n",
    "Start by creating or loading a dataset. For this example:'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'Age': [25, 30, 35, 40],\n",
    "    'Income': [50000, 60000, 55000, 45000]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original Data:\")\n",
    "print(df)\n",
    "\n",
    "'''2. Normalization (Min-Max Scaling)\n",
    "Scales the data to a fixed range, typically [0, 1].'''\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(df)\n",
    "normalized_df = pd.DataFrame(normalized_data, columns=df.columns)\n",
    "\n",
    "print(\"\\nNormalized Data:\")\n",
    "print(normalized_df)\n",
    "\n",
    "'''3. Standardization (Z-Score Scaling)\n",
    "Centers the data around zero with a standard deviation of 1.'''\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(df)\n",
    "standardized_df = pd.DataFrame(standardized_data, columns=df.columns)\n",
    "\n",
    "print(\"\\nStandardized Data:\")\n",
    "print(standardized_df)\n",
    "\n",
    "'''4. Robust Scaling\n",
    "Uses median and interquartile range (IQR), making it robust to outliers.'''\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "robust_scaled_data = scaler.fit_transform(df)\n",
    "robust_scaled_df = pd.DataFrame(robust_scaled_data, columns=df.columns)\n",
    "\n",
    "print(\"\\nRobust Scaled Data:\")\n",
    "print(robust_scaled_df)\n",
    "\n",
    "'''5. Custom Scaling (if needed)\n",
    "You can also implement scaling manually:\n",
    "\n",
    "Example: Min-Max Scaling'''\n",
    "\n",
    "min_val = df.min()\n",
    "max_val = df.max()\n",
    "custom_scaled_df = (df - min_val) / (max_val - min_val)\n",
    "\n",
    "print(\"\\nCustom Min-Max Scaled Data:\")\n",
    "print(custom_scaled_df)\n",
    "\n",
    "'''6. Scaling with Train-Test Split\n",
    "Always fit the scaler on the training data to avoid data leakage.'''\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data\n",
    "X_train, X_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit scaler on training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nScaled Training Data:\")\n",
    "print(X_train_scaled)\n",
    "print(\"\\nScaled Testing Data:\")\n",
    "print(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c44ba7c9-e2ba-4ed8-a977-a5ea00f6226a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample code for label encoding\n",
      "[2 0 1 0]\n",
      " \n",
      "Sample code for One-Host Encoding\n",
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]]\n",
      " \n",
      "Sample code for Ordinal Encoding\n",
      "[[0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]]\n",
      "\n",
      "Sample code for Binary Encoding\n",
      "   Color_0  Color_1  Color_2\n",
      "0        0        0        1\n",
      "1        0        1        0\n",
      "2        0        1        1\n",
      "3        1        0        0\n",
      "\n",
      "Sample code for Target Encoding\n",
      "   Color  Sales  Color_Encoded\n",
      "0    Red    100          112.5\n",
      "1   Blue    200          225.0\n",
      "2  Green    150          150.0\n",
      "3   Blue    250          225.0\n",
      "4    Red    125          112.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:411: FutureWarning: The `_get_tags` method is deprecated in 1.6 and will be removed in 1.7. Please implement the `__sklearn_tags__` method.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:411: FutureWarning: The `_get_tags` method is deprecated in 1.6 and will be removed in 1.7. Please implement the `__sklearn_tags__` method.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "'''Q.25] Explain data encoding?\n",
    "Ans. Data encoding is the process of converting categorical data into a numerical format, allowing machine learning models to work with this data. \n",
    "Most machine learning algorithms require numerical input, so encoding categorical variables ensures the model can properly interpret and learn from \n",
    "the data.\n",
    "\n",
    "Common types of data encoding techniques:\n",
    "\n",
    "1. Label Encoding\n",
    "Label encoding converts each category in a feature into a unique integer label.\n",
    "\n",
    "Example:\n",
    "Suppose you have a feature Color with categories: Red, Blue, Green.\n",
    "\n",
    "Before Encoding:\n",
    "Color\n",
    "    Red\n",
    "    Blue\n",
    "    Green\n",
    "\n",
    "After Label Encoding:\n",
    "Color\n",
    "    0 (Red)\n",
    "    1 (Blue)\n",
    "    2 (Green)\n",
    "\n",
    "Use case: Suitable for ordinal (ordered) categories, where the order matters, such as \"Low\", \"Medium\", and \"High\".'''\n",
    "\n",
    "print(\"Sample code for label encoding\")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Sample data\n",
    "data = ['Red', 'Blue', 'Green', 'Blue']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoded_data = encoder.fit_transform(data)\n",
    "\n",
    "print(encoded_data)\n",
    "\n",
    "'''2. One-Hot Encoding\n",
    "One-hot encoding creates binary columns for each category. Each column corresponds to a category in the feature, and the value is 1 if the category \n",
    "is present, otherwise 0.\n",
    "\n",
    "Example:\n",
    "For the same Color feature:\n",
    "\n",
    "Before Encoding:\n",
    "Color\n",
    "    Red\n",
    "    Blue\n",
    "    Green\n",
    "\n",
    "After One-Hot Encoding:\n",
    "\n",
    "Red  Blue  Green\n",
    "1     0     0\n",
    "0     1     0\n",
    "0     0     1\n",
    "\n",
    "Use case: Suitable for nominal (unordered) categories, like Color, Country, etc., where no order is implied.'''\n",
    "\n",
    "print(\" \")\n",
    "print(\"Sample code for One-Host Encoding\")\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = ['Red', 'Blue', 'Green', 'Blue']\n",
    "data = np.array(data).reshape(-1, 1)\n",
    "\n",
    "# If you're using scikit-learn v0.24 or later\n",
    "encoder = OneHotEncoder(sparse_output=False)  # Use sparse_output instead of sparse\n",
    "encoded_data = encoder.fit_transform(data)\n",
    "\n",
    "print(encoded_data)\n",
    "\n",
    "'''3. Ordinal Encoding\n",
    "Ordinal encoding is a variation of label encoding used for ordinal (ordered) categories where the relative order is important.\n",
    "\n",
    "Example:\n",
    "For a Size feature with categories: Small, Medium, Large:\n",
    "\n",
    "Before Encoding:\n",
    "Size\n",
    "    Small\n",
    "    Medium\n",
    "    Large\n",
    "\n",
    "After Ordinal Encoding:\n",
    "Size\n",
    "    0 (Small)\n",
    "    1 (Medium)\n",
    "    2 (Large)\n",
    "    \n",
    "You can manually map the order if needed, but OrdinalEncoder from sklearn is often used.'''\n",
    "\n",
    "print(\" \")\n",
    "print(\"Sample code for Ordinal Encoding\")\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Sample data\n",
    "data = ['Small', 'Medium', 'Large', 'Medium']\n",
    "\n",
    "encoder = OrdinalEncoder(categories=[['Small', 'Medium', 'Large']])\n",
    "encoded_data = encoder.fit_transform(np.array(data).reshape(-1, 1))\n",
    "\n",
    "print(encoded_data)\n",
    "\n",
    "'''4. Binary Encoding\n",
    "Binary encoding is a compromise between label encoding and one-hot encoding. It converts categories into binary numbers, reducing the number of \n",
    "columns needed for high-cardinality categorical variables.\n",
    "\n",
    "Example:\n",
    "For a Color feature with categories: Red, Blue, Green, Yellow:\n",
    "\n",
    "Before Encoding:\n",
    "Color\n",
    "    Red\n",
    "    Blue\n",
    "    Green\n",
    "    Yellow\n",
    "\n",
    "After Binary Encoding:\n",
    "    Color  Binary Code\n",
    "    Red    00\n",
    "    Blue   01\n",
    "    Green  10\n",
    "    Yellow 11\n",
    "\n",
    "This approach reduces dimensionality compared to one-hot encoding, especially when you have a lot of categories.'''\n",
    "\n",
    "print(\"\\nSample code for Binary Encoding\")\n",
    "from category_encoders import BinaryEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = ['Red', 'Blue', 'Green', 'Yellow']\n",
    "\n",
    "encoder = BinaryEncoder()\n",
    "encoded_data = encoder.fit_transform(pd.DataFrame(data, columns=['Color']))\n",
    "\n",
    "print(encoded_data)\n",
    "\n",
    "'''5. Target Encoding (Mean Encoding)\n",
    "Target encoding replaces categories with the mean of the target variable for each category. This method is particularly useful when dealing with \n",
    "high-cardinality features.\n",
    "\n",
    "Example:\n",
    "For a Color feature and a target variable Sales:\n",
    "\n",
    "Before Encoding:\n",
    "Color  Sales\n",
    "Red    100\n",
    "Blue   200\n",
    "Green  150\n",
    "Blue   250\n",
    "\n",
    "After Target Encoding:\n",
    "Color  Sales\n",
    "Red    100\n",
    "Blue   225  (mean of 200 and 250)\n",
    "Green  150\n",
    "'''\n",
    "\n",
    "print(\"\\nSample code for Target Encoding\")\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'Color': ['Red', 'Blue', 'Green', 'Blue', 'Red'],\n",
    "    'Sales': [100, 200, 150, 250, 125]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply Target Encoding\n",
    "encoding = df.groupby('Color')['Sales'].mean()\n",
    "df['Color_Encoded'] = df['Color'].map(encoding)\n",
    "\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81eff1c0-9004-47e3-b430-5de77c777bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae60715-3527-4135-a67d-042ef756ff17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
